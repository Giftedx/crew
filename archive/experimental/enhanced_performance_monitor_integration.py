#!/usr/bin/env python3
"""
Enhanced Performance Monitor Demo

Demonstrates the logical next step: integrating AI routing intelligence
with the existing performance monitoring system for production optimization.
"""

import asyncio
import json
import logging
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class AIRoutingMetrics:
    """AI routing performance metrics for enhanced monitoring."""

    routing_strategy: str
    model_selection_accuracy: float
    cost_optimization_score: float
    latency_optimization_score: float
    quality_optimization_score: float
    routing_confidence: float
    adaptive_learning_progress: float
    model_usage_distribution: dict[str, float]
    optimization_effectiveness: float


@dataclass
class PerformanceMetric:
    """Standard performance metric."""

    metric_name: str
    target_value: float
    actual_value: float
    trend: str
    confidence: float


@dataclass
class EnhancedPerformanceReport:
    """Enhanced performance report with AI routing intelligence."""

    agent_name: str
    reporting_period: dict[str, str]
    overall_score: float
    standard_metrics: list[PerformanceMetric]
    ai_routing_metrics: AIRoutingMetrics | None
    ai_enhancement_score: float
    recommendations: list[str]
    model_performance_summary: dict[str, Any]


class EnhancedPerformanceMonitor:
    """Enhanced performance monitor with AI routing integration."""

    def __init__(self, data_dir: Path | None = None):
        self.data_dir = data_dir or Path("data/enhanced_performance")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # Interaction history
        self.agent_interactions: dict[str, list[dict]] = defaultdict(list)
        self.ai_routing_history: dict[str, list[dict]] = defaultdict(list)

        # Performance thresholds
        self.ai_thresholds = {
            "routing_confidence": 0.80,
            "model_selection_accuracy": 0.85,
            "cost_optimization": 0.70,
            "quality_optimization": 0.80,
            "learning_progress": 0.10,
        }

        logger.info("‚úÖ Enhanced Performance Monitor initialized")

    def record_standard_interaction(
        self,
        agent_name: str,
        task_type: str,
        tools_used: list[str],
        response_quality: float,
        response_time: float,
        user_feedback: dict[str, Any] | None = None,
    ):
        """Record a standard agent interaction."""

        interaction = {
            "timestamp": datetime.now().isoformat(),
            "agent_name": agent_name,
            "task_type": task_type,
            "tools_used": tools_used,
            "response_quality": response_quality,
            "response_time": response_time,
            "user_feedback": user_feedback or {},
        }

        self.agent_interactions[agent_name].append(interaction)
        logger.info(f"Standard interaction recorded: {agent_name} (quality: {response_quality:.2f})")

    def record_ai_routing_interaction(
        self,
        agent_name: str,
        task_type: str,
        routing_strategy: str,
        selected_model: str,
        routing_confidence: float,
        expected_performance: dict[str, float],
        actual_performance: dict[str, float],
        optimization_target: str = "balanced",
        user_feedback: dict[str, Any] | None = None,
    ):
        """Record an AI routing interaction with enhanced metrics."""

        # Also record as standard interaction
        self.record_standard_interaction(
            agent_name=agent_name,
            task_type=task_type,
            tools_used=[
                f"ai_router_{routing_strategy}",
                f"model_{selected_model.split('/')[-1]}",
            ],
            response_quality=actual_performance.get("quality", 0.0),
            response_time=actual_performance.get("latency_ms", 0.0) / 1000.0,
            user_feedback=user_feedback,
        )

        # Record AI routing specific data
        ai_interaction = {
            "timestamp": datetime.now().isoformat(),
            "agent_name": agent_name,
            "task_type": task_type,
            "routing_strategy": routing_strategy,
            "selected_model": selected_model,
            "routing_confidence": routing_confidence,
            "optimization_target": optimization_target,
            "expected_performance": expected_performance,
            "actual_performance": actual_performance,
            "performance_delta": {
                "latency": actual_performance.get("latency_ms", 0) - expected_performance.get("latency_ms", 0),
                "cost": actual_performance.get("cost", 0) - expected_performance.get("cost", 0),
                "quality": actual_performance.get("quality", 0) - expected_performance.get("quality", 0),
            },
            "user_feedback": user_feedback or {},
        }

        self.ai_routing_history[agent_name].append(ai_interaction)

        logger.info(
            f"AI routing interaction recorded: {agent_name} -> {selected_model} (confidence: {routing_confidence:.2f})"
        )

    def calculate_ai_routing_metrics(self, agent_name: str, days: int = 7) -> AIRoutingMetrics | None:
        """Calculate comprehensive AI routing performance metrics."""

        if agent_name not in self.ai_routing_history:
            return None

        cutoff_date = datetime.now() - timedelta(days=days)
        recent_interactions = [
            interaction
            for interaction in self.ai_routing_history[agent_name]
            if datetime.fromisoformat(interaction["timestamp"]) > cutoff_date
        ]

        if not recent_interactions:
            return None

        # Calculate metrics
        strategy_counts = defaultdict(int)
        total_confidence = 0.0
        model_usage = defaultdict(int)
        optimization_scores = {"cost": [], "latency": [], "quality": []}
        accuracy_samples = []

        for interaction in recent_interactions:
            strategy_counts[interaction["routing_strategy"]] += 1
            total_confidence += interaction["routing_confidence"]
            model_usage[interaction["selected_model"]] += 1

            # Calculate optimization effectiveness
            expected = interaction["expected_performance"]
            actual = interaction["actual_performance"]

            # Cost optimization (lower actual vs expected = better)
            if expected.get("cost", 0) > 0:
                cost_ratio = min(2.0, expected["cost"] / max(actual.get("cost", 0.001), 0.001))
                optimization_scores["cost"].append(min(1.0, cost_ratio))

            # Latency optimization
            if expected.get("latency_ms", 0) > 0:
                latency_ratio = min(2.0, expected["latency_ms"] / max(actual.get("latency_ms", 1), 1))
                optimization_scores["latency"].append(min(1.0, latency_ratio))

            # Quality optimization
            expected_quality = expected.get("quality", 0.5)
            actual_quality = actual.get("quality", 0.0)
            if expected_quality > 0:
                quality_ratio = min(2.0, actual_quality / expected_quality)
                optimization_scores["quality"].append(min(1.0, quality_ratio))

            # Model selection accuracy
            model_accuracy = actual_quality * interaction["routing_confidence"]
            accuracy_samples.append(model_accuracy)

        # Calculate final metrics
        primary_strategy = max(strategy_counts.keys(), key=strategy_counts.get) if strategy_counts else "unknown"
        avg_confidence = total_confidence / len(recent_interactions)
        model_accuracy = sum(accuracy_samples) / len(accuracy_samples) if accuracy_samples else 0.0

        cost_score = (
            sum(optimization_scores["cost"]) / len(optimization_scores["cost"]) if optimization_scores["cost"] else 0.0
        )
        latency_score = (
            sum(optimization_scores["latency"]) / len(optimization_scores["latency"])
            if optimization_scores["latency"]
            else 0.0
        )
        quality_score = (
            sum(optimization_scores["quality"]) / len(optimization_scores["quality"])
            if optimization_scores["quality"]
            else 0.0
        )

        # Learning progress (simplified)
        learning_progress = self._calculate_learning_progress(recent_interactions)

        # Model usage distribution
        total_usage = sum(model_usage.values())
        usage_distribution = {model: count / total_usage for model, count in model_usage.items()}

        # Optimization effectiveness
        optimization_effectiveness = (cost_score + latency_score + quality_score) / 3

        return AIRoutingMetrics(
            routing_strategy=primary_strategy,
            model_selection_accuracy=model_accuracy,
            cost_optimization_score=cost_score,
            latency_optimization_score=latency_score,
            quality_optimization_score=quality_score,
            routing_confidence=avg_confidence,
            adaptive_learning_progress=learning_progress,
            model_usage_distribution=usage_distribution,
            optimization_effectiveness=optimization_effectiveness,
        )

    def _calculate_learning_progress(self, interactions: list[dict]) -> float:
        """Calculate learning progress over time."""

        if len(interactions) < 6:
            return 0.0

        # Compare first third vs last third performance
        third_size = len(interactions) // 3
        first_third = interactions[:third_size]
        last_third = interactions[-third_size:]

        def avg_performance(group):
            scores = [i["actual_performance"].get("quality", 0) * i["routing_confidence"] for i in group]
            return sum(scores) / len(scores) if scores else 0.0

        first_perf = avg_performance(first_third)
        last_perf = avg_performance(last_third)

        if first_perf > 0:
            improvement = (last_perf - first_perf) / first_perf
            return max(0.0, min(1.0, improvement + 0.5))  # Normalize

        return 0.0

    def generate_optimization_recommendations(self, ai_metrics: AIRoutingMetrics) -> list[str]:
        """Generate optimization recommendations based on AI routing performance."""

        recommendations = []

        if ai_metrics.routing_confidence < self.ai_thresholds["routing_confidence"]:
            recommendations.append(
                f"üéØ IMPROVE ROUTING CONFIDENCE: Current {ai_metrics.routing_confidence:.2f} < "
                f"target {self.ai_thresholds['routing_confidence']:.2f}. Enable adaptive learning."
            )

        if ai_metrics.model_selection_accuracy < self.ai_thresholds["model_selection_accuracy"]:
            recommendations.append(
                f"ü§ñ ENHANCE MODEL SELECTION: Current accuracy {ai_metrics.model_selection_accuracy:.2f} < "
                f"target {self.ai_thresholds['model_selection_accuracy']:.2f}. Review task matching."
            )

        if ai_metrics.cost_optimization_score < self.ai_thresholds["cost_optimization"]:
            recommendations.append(
                f"üí∞ OPTIMIZE COSTS: Current score {ai_metrics.cost_optimization_score:.2f} < "
                f"target {self.ai_thresholds['cost_optimization']:.2f}. Enable cost-aware routing."
            )

        if ai_metrics.quality_optimization_score < self.ai_thresholds["quality_optimization"]:
            recommendations.append(
                f"‚≠ê IMPROVE QUALITY: Current score {ai_metrics.quality_optimization_score:.2f} < "
                f"target {self.ai_thresholds['quality_optimization']:.2f}. Prioritize quality routing."
            )

        if ai_metrics.adaptive_learning_progress < self.ai_thresholds["learning_progress"]:
            recommendations.append(
                f"üìö ACCELERATE LEARNING: Current rate {ai_metrics.adaptive_learning_progress:.2f} < "
                f"target {self.ai_thresholds['learning_progress']:.2f}. Increase feedback collection."
            )

        return recommendations

    def analyze_model_performance(self, agent_name: str) -> dict[str, Any]:
        """Analyze individual model performance."""

        if agent_name not in self.ai_routing_history:
            return {}

        model_stats = defaultdict(
            lambda: {
                "usage_count": 0,
                "quality_scores": [],
                "costs": [],
                "latencies": [],
                "success_rate": 0.0,
            }
        )

        for interaction in self.ai_routing_history[agent_name]:
            model = interaction["selected_model"]
            actual = interaction["actual_performance"]

            model_stats[model]["usage_count"] += 1
            model_stats[model]["quality_scores"].append(actual.get("quality", 0))
            model_stats[model]["costs"].append(actual.get("cost", 0))
            model_stats[model]["latencies"].append(actual.get("latency_ms", 0))

        # Calculate averages
        model_performance = {}
        for model, stats in model_stats.items():
            if stats["usage_count"] > 0:
                model_performance[model] = {
                    "usage_count": stats["usage_count"],
                    "avg_quality": sum(stats["quality_scores"]) / len(stats["quality_scores"]),
                    "avg_cost": sum(stats["costs"]) / len(stats["costs"]),
                    "avg_latency": sum(stats["latencies"]) / len(stats["latencies"]),
                }

        return model_performance

    def generate_enhanced_report(self, agent_name: str, days: int = 7) -> EnhancedPerformanceReport:
        """Generate comprehensive enhanced performance report."""

        # Calculate standard metrics (simplified)
        standard_interactions = self.agent_interactions.get(agent_name, [])
        if standard_interactions:
            avg_quality = sum(i["response_quality"] for i in standard_interactions) / len(standard_interactions)
            avg_response_time = sum(i["response_time"] for i in standard_interactions) / len(standard_interactions)
        else:
            avg_quality = 0.0
            avg_response_time = 0.0

        standard_metrics = [
            PerformanceMetric("response_quality", 0.85, avg_quality, "stable", 0.8),
            PerformanceMetric("response_time", 30.0, avg_response_time, "stable", 0.8),
        ]

        # Calculate AI routing metrics
        ai_metrics = self.calculate_ai_routing_metrics(agent_name, days)

        # Calculate AI enhancement score
        ai_enhancement_score = 0.0
        if ai_metrics:
            ai_enhancement_score = (
                ai_metrics.routing_confidence * 0.25
                + ai_metrics.model_selection_accuracy * 0.25
                + ai_metrics.optimization_effectiveness * 0.30
                + ai_metrics.adaptive_learning_progress * 0.20
            )

        # Generate recommendations
        recommendations = []
        if ai_metrics:
            recommendations = self.generate_optimization_recommendations(ai_metrics)

        # Analyze model performance
        model_performance = self.analyze_model_performance(agent_name)

        # Calculate overall score
        overall_score = (avg_quality + ai_enhancement_score) / 2 if ai_metrics else avg_quality

        return EnhancedPerformanceReport(
            agent_name=agent_name,
            reporting_period={
                "start_date": (datetime.now() - timedelta(days=days)).isoformat(),
                "end_date": datetime.now().isoformat(),
                "days_analyzed": str(days),
            },
            overall_score=overall_score,
            standard_metrics=standard_metrics,
            ai_routing_metrics=ai_metrics,
            ai_enhancement_score=ai_enhancement_score,
            recommendations=recommendations,
            model_performance_summary=model_performance,
        )

    def run_comprehensive_analysis(self, agent_name: str) -> dict[str, Any]:
        """Run comprehensive analysis of agent performance."""

        print(f"üîç ENHANCED PERFORMANCE ANALYSIS: {agent_name}")
        print("=" * 60)

        # Generate enhanced report
        report = self.generate_enhanced_report(agent_name)

        print("üìä Standard Performance Metrics:")
        for metric in report.standard_metrics:
            status = "‚úÖ" if metric.actual_value >= metric.target_value else "‚ö†Ô∏è"
            print(f"   {status} {metric.metric_name}: {metric.actual_value:.2f} (target: {metric.target_value:.2f})")

        if report.ai_routing_metrics:
            ai_metrics = report.ai_routing_metrics
            print("\nü§ñ AI Routing Performance:")
            print(f"   ‚Ä¢ Strategy: {ai_metrics.routing_strategy}")
            print(f"   ‚Ä¢ Routing Confidence: {ai_metrics.routing_confidence:.3f}")
            print(f"   ‚Ä¢ Model Selection Accuracy: {ai_metrics.model_selection_accuracy:.3f}")
            print(f"   ‚Ä¢ Cost Optimization: {ai_metrics.cost_optimization_score:.3f}")
            print(f"   ‚Ä¢ Quality Optimization: {ai_metrics.quality_optimization_score:.3f}")
            print(f"   ‚Ä¢ Learning Progress: {ai_metrics.adaptive_learning_progress:.3f}")

            print("\nüìä Model Usage Distribution:")
            for model, percentage in ai_metrics.model_usage_distribution.items():
                model_name = model.split("/")[-1] if "/" in model else model
                print(f"   ‚Ä¢ {model_name}: {percentage:.1%}")

        print("\nüìà Performance Summary:")
        print(f"   ‚Ä¢ Overall Score: {report.overall_score:.3f}")
        print(f"   ‚Ä¢ AI Enhancement Score: {report.ai_enhancement_score:.3f}")
        print(f"   ‚Ä¢ Standard Interactions: {len(self.agent_interactions.get(agent_name, []))}")
        print(f"   ‚Ä¢ AI Routing Interactions: {len(self.ai_routing_history.get(agent_name, []))}")

        if report.recommendations:
            print("\nüí° Optimization Recommendations:")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"   {i}. {rec}")

        # Status assessment
        if report.overall_score >= 0.85:
            status = "EXCELLENT"
            print("\n‚ú® Performance is excellent!")
        elif report.overall_score >= 0.70:
            status = "GOOD"
            print("\n‚úÖ Performance is good with optimization opportunities.")
        else:
            status = "NEEDS_IMPROVEMENT"
            print("\n‚ö†Ô∏è Performance needs improvement.")

        return {
            "status": status,
            "report": report,
            "ai_routing_enabled": bool(report.ai_routing_metrics),
        }


async def demonstrate_enhanced_performance_monitoring():
    """Demonstrate the enhanced performance monitoring with AI routing integration."""

    print("üöÄ ENHANCED PERFORMANCE MONITORING INTEGRATION")
    print("=" * 70)
    print("The most logical next step: Integrating AI routing with performance monitoring")
    print()

    # Initialize enhanced monitor
    monitor = EnhancedPerformanceMonitor()
    agent_name = "enhanced_ai_agent"

    print("üìä Simulating agent interactions...")

    # Record standard agent interactions
    standard_scenarios = [
        (
            "fact_verification",
            ["claim_extractor", "fact_checker", "vector_search"],
            0.85,
            12.5,
        ),
        ("content_analysis", ["text_analyzer", "sentiment_tool"], 0.78, 8.2),
        ("research_task", ["web_search", "summarizer", "fact_checker"], 0.92, 15.7),
    ]

    for task_type, tools, quality, time in standard_scenarios:
        monitor.record_standard_interaction(
            agent_name=agent_name,
            task_type=task_type,
            tools_used=tools,
            response_quality=quality,
            response_time=time,
            user_feedback={"satisfaction": quality + 0.05},
        )
        await asyncio.sleep(0.1)

    print("ü§ñ Simulating AI routing interactions...")

    # Record AI routing interactions
    ai_scenarios = [
        (
            "analysis",
            "adaptive_learning",
            "anthropic/claude-3-5-sonnet-20241022",
            0.92,
            {"latency_ms": 1800, "cost": 0.015, "quality": 0.90},
            {"latency_ms": 1750, "cost": 0.014, "quality": 0.93, "success": True},
        ),
        (
            "general",
            "performance_based",
            "openai/gpt-4o",
            0.85,
            {"latency_ms": 1200, "cost": 0.005, "quality": 0.85},
            {"latency_ms": 1150, "cost": 0.0048, "quality": 0.87, "success": True},
        ),
        (
            "fast",
            "speed_optimized",
            "google/gemini-1.5-flash",
            0.78,
            {"latency_ms": 600, "cost": 0.001, "quality": 0.75},
            {"latency_ms": 580, "cost": 0.0009, "quality": 0.78, "success": True},
        ),
        (
            "cost",
            "cost_optimized",
            "openai/gpt-4o-mini",
            0.82,
            {"latency_ms": 900, "cost": 0.002, "quality": 0.80},
            {"latency_ms": 920, "cost": 0.0018, "quality": 0.82, "success": True},
        ),
    ]

    for task_type, strategy, model, confidence, expected, actual in ai_scenarios:
        monitor.record_ai_routing_interaction(
            agent_name=agent_name,
            task_type=task_type,
            routing_strategy=strategy,
            selected_model=model,
            routing_confidence=confidence,
            expected_performance=expected,
            actual_performance=actual,
            optimization_target=strategy.replace("_optimized", "").replace("_based", ""),
            user_feedback={"satisfaction": 0.8 + (hash(model) % 10) / 50},
        )
        await asyncio.sleep(0.1)

    print("‚úÖ Simulation complete!")
    print()

    # Run comprehensive analysis
    analysis_result = monitor.run_comprehensive_analysis(agent_name)

    print("\nüéØ INTEGRATION SUCCESS:")
    print(f"   ‚Ä¢ Status: {analysis_result['status']}")
    print(f"   ‚Ä¢ AI Routing Integration: {'‚úÖ ENABLED' if analysis_result['ai_routing_enabled'] else '‚ùå DISABLED'}")
    print("   ‚Ä¢ Enhanced Monitoring: ‚úÖ OPERATIONAL")

    print("\nüîÑ PRODUCTION READINESS:")
    print("   1. ‚úÖ Standard performance monitoring maintained")
    print("   2. ‚úÖ AI routing intelligence integrated")
    print("   3. ‚úÖ Enhanced metrics and recommendations")
    print("   4. ‚úÖ Model performance analysis")
    print("   5. ‚úÖ Optimization suggestions generated")

    print("\n‚ú® LOGICAL PROGRESSION COMPLETE:")
    print("   Phase 1: ‚úÖ Type Safety Foundation")
    print("   Phase 2: ‚úÖ Test Reliability Enhancement")
    print("   Phase 3: ‚úÖ Performance Optimization")
    print("   Phase 4: ‚úÖ AI Enhancement (LiteLLM Router)")
    print("   Phase 5: ‚úÖ Performance Monitoring Integration")

    # Save results
    results = {
        "integration_status": "ENHANCED_PERFORMANCE_MONITORING_COMPLETE",
        "analysis_result": {
            "status": analysis_result["status"],
            "ai_routing_enabled": analysis_result["ai_routing_enabled"],
            "overall_score": analysis_result["report"].overall_score,
            "ai_enhancement_score": analysis_result["report"].ai_enhancement_score,
        },
        "next_phase": "production_deployment_optimization",
        "completion_time": datetime.now().isoformat(),
    }

    try:
        with open("/home/crew/enhanced_performance_monitoring_results.json", "w") as f:
            json.dump(results, f, indent=2)
        print("\nüíæ Results saved to enhanced_performance_monitoring_results.json")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save results: {e}")

    return analysis_result


if __name__ == "__main__":
    asyncio.run(demonstrate_enhanced_performance_monitoring())
