---
description: "Database schema design, migration patterns, and data access patterns"
alwaysApply: false
---

# Database Schema & Migration Patterns - Ultimate Discord Intelligence Bot

## Schema Design Principles

### Core Tables

#### Content Table

```sql
CREATE TABLE content (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant VARCHAR(255) NOT NULL,
    workspace VARCHAR(255) NOT NULL,
    content_hash VARCHAR(64) NOT NULL,  -- SHA-256 hash for deduplication
    content_type VARCHAR(50) NOT NULL,  -- 'text', 'url', 'file', 'discord_message'
    source_url TEXT,                    -- Original source URL
    title TEXT,
    content TEXT NOT NULL,
    metadata JSONB,                     -- Flexible metadata storage
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),

    -- Indexes for performance
    UNIQUE(tenant, workspace, content_hash),
    INDEX idx_content_tenant_workspace (tenant, workspace),
    INDEX idx_content_type (content_type),
    INDEX idx_content_created_at (created_at DESC)
);

-- Trigger for updated_at
CREATE TRIGGER update_content_updated_at
    BEFORE UPDATE ON content
    FOR EACH ROW EXECUTE FUNCTION trigger_set_timestamp();
```

#### Content Analysis Table

```sql
CREATE TABLE content_analysis (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID NOT NULL REFERENCES content(id) ON DELETE CASCADE,
    tenant VARCHAR(255) NOT NULL,
    workspace VARCHAR(255) NOT NULL,
    analysis_type VARCHAR(50) NOT NULL, -- 'sentiment', 'topics', 'summary', 'entities'
    analysis_result JSONB NOT NULL,
    confidence_score DECIMAL(3,2),      -- 0.00 to 1.00
    processing_time_ms INTEGER,
    model_used VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW(),

    -- Indexes
    INDEX idx_analysis_content_id (content_id),
    INDEX idx_analysis_tenant_workspace (tenant, workspace),
    INDEX idx_analysis_type (analysis_type),
    INDEX idx_analysis_created_at (created_at DESC)
);
```

#### Memory Vectors Table

```sql
CREATE TABLE memory_vectors (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    content_id UUID NOT NULL REFERENCES content(id) ON DELETE CASCADE,
    tenant VARCHAR(255) NOT NULL,
    workspace VARCHAR(255) NOT NULL,
    vector_type VARCHAR(50) NOT NULL,   -- 'content', 'analysis', 'user_query'
    vector_data VECTOR(1536),           -- OpenAI embedding dimension
    metadata JSONB,                     -- Additional vector metadata
    created_at TIMESTAMPTZ DEFAULT NOW(),

    -- Indexes for vector search performance
    INDEX idx_vectors_tenant_workspace (tenant, workspace),
    INDEX idx_vectors_type (vector_type),
    INDEX idx_vectors_content_id (content_id)
);

-- Enable vector extension if needed
CREATE EXTENSION IF NOT EXISTS vector;
```

#### Discord Messages Table

```sql
CREATE TABLE discord_messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    discord_message_id BIGINT NOT NULL UNIQUE,
    discord_channel_id BIGINT NOT NULL,
    discord_guild_id BIGINT NOT NULL,
    tenant VARCHAR(255) NOT NULL,
    workspace VARCHAR(255) NOT NULL,
    author_id BIGINT NOT NULL,
    author_username VARCHAR(255),
    content TEXT,
    message_type VARCHAR(50),           -- 'text', 'image', 'file', 'embed'
    attachments JSONB,                  -- File attachment metadata
    embeds JSONB,                       -- Rich embed data
    reactions JSONB,                    -- Emoji reactions
    created_at TIMESTAMPTZ DEFAULT NOW(),
    processed_at TIMESTAMPTZ,           -- When bot processed this message

    -- Indexes
    INDEX idx_discord_message_id (discord_message_id),
    INDEX idx_discord_channel (discord_channel_id),
    INDEX idx_discord_guild (discord_guild_id),
    INDEX idx_discord_tenant_workspace (tenant, workspace),
    INDEX idx_discord_created_at (created_at DESC)
);
```

## Migration Patterns

### Migration File Structure

```python
# alembic/versions/001_initial_schema.py
"""Initial schema

Revision ID: 001
Revises: None
Create Date: 2025-01-01 12:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB, UUID

# revision identifiers, used by Alembic
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    """Create initial schema."""
    # Content table
    op.create_table('content',
        sa.Column('id', UUID(), nullable=False),
        sa.Column('tenant', sa.String(255), nullable=False),
        sa.Column('workspace', sa.String(255), nullable=False),
        sa.Column('content_hash', sa.String(64), nullable=False),
        sa.Column('content_type', sa.String(50), nullable=False),
        sa.Column('source_url', sa.Text(), nullable=True),
        sa.Column('title', sa.Text(), nullable=True),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('metadata', JSONB(), nullable=True),
        sa.Column('created_at', sa.DateTime(), server_default=sa.text('NOW()'), nullable=False),
        sa.Column('updated_at', sa.DateTime(), server_default=sa.text('NOW()'), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('tenant', 'workspace', 'content_hash')
    )

    # Create indexes
    op.create_index('idx_content_tenant_workspace', 'content', ['tenant', 'workspace'])
    op.create_index('idx_content_type', 'content', ['content_type'])
    op.create_index('idx_content_created_at', 'content', ['created_at'])

    # Add updated_at trigger
    op.execute("""
        CREATE TRIGGER update_content_updated_at
            BEFORE UPDATE ON content
            FOR EACH ROW EXECUTE FUNCTION trigger_set_timestamp();
    """)

def downgrade():
    """Drop initial schema."""
    op.drop_table('content')
```

### Migration Best Practices

#### Zero-Downtime Migrations

```python
def upgrade():
    """Add new column with default value."""
    # Add column as nullable first
    op.add_column('content', sa.Column('processing_status', sa.String(50)))

    # Backfill existing rows in batches
    batch_size = 1000
    offset = 0

    while True:
        result = op.get_bind().execute(sa.text("""
            SELECT id FROM content
            WHERE processing_status IS NULL
            ORDER BY id
            LIMIT :batch_size OFFSET :offset
        """), batch_size=batch_size, offset=offset)

        ids = [row[0] for row in result]
        if not ids:
            break

        # Update batch
        op.execute(sa.text("""
            UPDATE content
            SET processing_status = 'pending'
            WHERE id IN :ids
        """), ids=tuple(ids))

        offset += batch_size

    # Make column not nullable
    op.alter_column('content', 'processing_status', nullable=False)

    # Add index for new column
    op.create_index('idx_content_processing_status', 'content', ['processing_status'])

def downgrade():
    """Remove new column."""
    op.drop_index('idx_content_processing_status', table_name='content')
    op.drop_column('content', 'processing_status')
```

#### Index Management

```python
def upgrade():
    """Add performance indexes."""
    # Create partial index for active content only
    op.execute("""
        CREATE INDEX CONCURRENTLY idx_content_active
        ON content (tenant, workspace, created_at DESC)
        WHERE processing_status = 'completed'
    """)

    # Create covering index for common queries
    op.execute("""
        CREATE INDEX CONCURRENTLY idx_content_search
        ON content (tenant, workspace, content_type, created_at DESC)
        INCLUDE (title, content)
    """)

def downgrade():
    """Remove performance indexes."""
    op.drop_index('idx_content_search', table_name='content')
    op.drop_index('idx_content_active', table_name='content')
```

## Data Access Patterns

### Repository Pattern

```python
from typing import List, Optional, Dict, Any
from sqlalchemy.orm import Session
from ultimate_discord_intelligence_bot.step_result import StepResult

class ContentRepository:
    """Repository for content data access."""

    def __init__(self, session: Session):
        self.session = session

    def get_by_id(self, content_id: str, tenant: str, workspace: str) -> StepResult:
        """Get content by ID."""
        try:
            content = self.session.query(Content).filter(
                Content.id == content_id,
                Content.tenant == tenant,
                Content.workspace == workspace
            ).first()

            if not content:
                return StepResult.fail("Content not found")

            return StepResult.ok(data=content.to_dict())

        except Exception as e:
            return StepResult.fail(f"Database error: {str(e)}")

    def get_by_hash(self, content_hash: str, tenant: str, workspace: str) -> StepResult:
        """Get content by hash for deduplication."""
        try:
            content = self.session.query(Content).filter(
                Content.content_hash == content_hash,
                Content.tenant == tenant,
                Content.workspace == workspace
            ).first()

            if content:
                return StepResult.ok(data=content.to_dict())

            return StepResult.ok(data=None)  # Not found is OK for deduplication

        except Exception as e:
            return StepResult.fail(f"Database error: {str(e)}")

    def create(self, content_data: Dict[str, Any], tenant: str, workspace: str) -> StepResult:
        """Create new content record."""
        try:
            content = Content(
                tenant=tenant,
                workspace=workspace,
                **content_data
            )

            self.session.add(content)
            self.session.commit()

            return StepResult.ok(data=content.to_dict())

        except Exception as e:
            self.session.rollback()
            return StepResult.fail(f"Failed to create content: {str(e)}")

    def search(self, tenant: str, workspace: str, filters: Dict[str, Any] = None,
               limit: int = 50, offset: int = 0) -> StepResult:
        """Search content with filters and pagination."""
        try:
            query = self.session.query(Content).filter(
                Content.tenant == tenant,
                Content.workspace == workspace
            )

            # Apply filters
            if filters:
                if filters.get('content_type'):
                    query = query.filter(Content.content_type == filters['content_type'])
                if filters.get('created_after'):
                    query = query.filter(Content.created_at >= filters['created_after'])

            # Apply pagination
            total = query.count()
            contents = query.order_by(Content.created_at.desc()) \
                           .limit(limit).offset(offset).all()

            return StepResult.ok(data={
                'items': [c.to_dict() for c in contents],
                'pagination': {
                    'total': total,
                    'limit': limit,
                    'offset': offset,
                    'has_more': offset + limit < total
                }
            })

        except Exception as e:
            return StepResult.fail(f"Search failed: {str(e)}")
```

### Vector Search Pattern

```python
class MemoryVectorRepository:
    """Repository for vector similarity search."""

    def __init__(self, session: Session):
        self.session = session

    def search_similar(self, query_vector: List[float], tenant: str, workspace: str,
                      limit: int = 10, threshold: float = 0.7) -> StepResult:
        """Find similar content using vector similarity."""
        try:
            # Using pgvector for similarity search
            results = self.session.execute(sa.text("""
                SELECT
                    mv.content_id,
                    mv.vector_type,
                    mv.metadata,
                    1 - (mv.vector_data <=> :query_vector) as similarity,
                    c.title,
                    c.content
                FROM memory_vectors mv
                JOIN content c ON mv.content_id = c.id
                WHERE mv.tenant = :tenant
                  AND mv.workspace = :workspace
                  AND 1 - (mv.vector_data <=> :query_vector) >= :threshold
                ORDER BY similarity DESC
                LIMIT :limit
            """), {
                'query_vector': query_vector,
                'tenant': tenant,
                'workspace': workspace,
                'threshold': threshold,
                'limit': limit
            })

            return StepResult.ok(data=[{
                'content_id': row.content_id,
                'vector_type': row.vector_type,
                'similarity': float(row.similarity),
                'title': row.title,
                'content': row.content[:200] + '...' if len(row.content) > 200 else row.content,
                'metadata': row.metadata
            } for row in results])

        except Exception as e:
            return StepResult.fail(f"Vector search failed: {str(e)}")

    def store_vector(self, content_id: str, vector_data: List[float],
                    vector_type: str, tenant: str, workspace: str,
                    metadata: Dict[str, Any] = None) -> StepResult:
        """Store vector embedding for content."""
        try:
            vector_record = MemoryVector(
                content_id=content_id,
                tenant=tenant,
                workspace=workspace,
                vector_type=vector_type,
                vector_data=vector_data,
                metadata=metadata or {}
            )

            self.session.add(vector_record)
            self.session.commit()

            return StepResult.ok(data=vector_record.id)

        except Exception as e:
            self.session.rollback()
            return StepResult.fail(f"Failed to store vector: {str(e)}")
```

### Connection Management

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from contextlib import contextmanager
import os

class DatabaseManager:
    """Database connection and session management."""

    def __init__(self):
        database_url = os.getenv('DATABASE_URL', 'postgresql://localhost/ultimate_bot')
        self.engine = create_engine(
            database_url,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)

    @contextmanager
    def get_session(self) -> Session:
        """Get database session with automatic cleanup."""
        session = self.SessionLocal()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()

    def health_check(self) -> bool:
        """Check database connectivity."""
        try:
            with self.get_session() as session:
                session.execute(sa.text("SELECT 1"))
            return True
        except Exception:
            return False

# Global database manager instance
db_manager = DatabaseManager()
```

## Backup & Recovery

### Automated Backup Pattern

```python
import boto3
from datetime import datetime, timezone
import subprocess

class DatabaseBackup:
    """Automated database backup management."""

    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.bucket_name = os.getenv('BACKUP_BUCKET', 'ultimate-bot-backups')

    def create_backup(self) -> StepResult:
        """Create full database backup."""
        try:
            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
            backup_file = f"backup_{timestamp}.sql"

            # Create backup using pg_dump
            cmd = [
                'pg_dump',
                '--no-owner',
                '--no-privileges',
                '--clean',
                '--if-exists',
                '--format=custom',
                os.getenv('DATABASE_URL')
            ]

            with open(f'/tmp/{backup_file}', 'wb') as f:
                subprocess.run(cmd, stdout=f, check=True)

            # Upload to S3
            self.s3_client.upload_file(
                f'/tmp/{backup_file}',
                self.bucket_name,
                f'backups/{backup_file}'
            )

            # Clean up local file
            os.remove(f'/tmp/{backup_file}')

            return StepResult.ok(data={'backup_file': backup_file})

        except Exception as e:
            return StepResult.fail(f"Backup failed: {str(e)}")

    def restore_backup(self, backup_file: str) -> StepResult:
        """Restore database from backup."""
        try:
            # Download from S3
            local_file = f'/tmp/{backup_file}'
            self.s3_client.download_file(
                self.bucket_name,
                f'backups/{backup_file}',
                local_file
            )

            # Restore using pg_restore
            cmd = [
                'pg_restore',
                '--no-owner',
                '--no-privileges',
                '--clean',
                '--if-exists',
                '--dbname', os.getenv('DATABASE_URL'),
                local_file
            ]

            subprocess.run(cmd, check=True)

            # Clean up local file
            os.remove(local_file)

            return StepResult.ok(data={'restored_from': backup_file})

        except Exception as e:
            return StepResult.fail(f"Restore failed: {str(e)}")
```

## Performance Optimization

### Query Optimization

```python
def optimize_queries():
    """Database query optimization patterns."""

    # Use EXPLAIN ANALYZE to identify slow queries
    slow_query = sa.text("""
        EXPLAIN (ANALYZE, BUFFERS)
        SELECT * FROM content
        WHERE tenant = :tenant AND workspace = :workspace
        ORDER BY created_at DESC
        LIMIT 100
    """)

    # Create composite indexes for common query patterns
    op.create_index(
        'idx_content_composite',
        'content',
        ['tenant', 'workspace', 'created_at'],
        postgresql_where=sa.text("processing_status = 'completed'")
    )

    # Use query result caching for frequently accessed data
    @lru_cache(maxsize=1000)
    def get_tenant_stats(tenant: str, workspace: str):
        """Cached tenant statistics."""
        with db_manager.get_session() as session:
            result = session.execute(sa.text("""
                SELECT
                    COUNT(*) as total_content,
                    COUNT(DISTINCT content_type) as content_types,
                    MAX(created_at) as latest_content
                FROM content
                WHERE tenant = :tenant AND workspace = :workspace
            """), {'tenant': tenant, 'workspace': workspace})

            return result.first()
```

## Data Retention & Cleanup

### Automated Cleanup Pattern

```python
class DataRetentionManager:
    """Automated data retention and cleanup."""

    def __init__(self):
        self.retention_days = {
            'content': 90,      # Keep content for 90 days
            'analysis': 30,     # Keep analysis for 30 days
            'logs': 7,          # Keep logs for 7 days
            'temp_data': 1      # Keep temp data for 1 day
        }

    def cleanup_old_data(self) -> StepResult:
        """Clean up data older than retention period."""
        try:
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=min(self.retention_days.values()))

            with db_manager.get_session() as session:
                # Delete old content
                session.execute(sa.text("""
                    DELETE FROM content
                    WHERE created_at < :cutoff
                      AND content_type != 'discord_message'
                """), {'cutoff': cutoff_date})

                # Delete orphaned analysis records
                session.execute(sa.text("""
                    DELETE FROM content_analysis ca
                    WHERE NOT EXISTS (
                        SELECT 1 FROM content c WHERE c.id = ca.content_id
                    )
                """))

                # Delete old logs
                session.execute(sa.text("""
                    DELETE FROM application_logs
                    WHERE created_at < :cutoff
                """), {'cutoff': cutoff_date})

            return StepResult.ok(data={'cleanup_date': cutoff_date})

        except Exception as e:
            return StepResult.fail(f"Cleanup failed: {str(e)}")
```

## Transaction Management

### Complex Transaction Pattern

```python
def process_content_with_analysis(content_data: Dict[str, Any],
                                tenant: str, workspace: str) -> StepResult:
    """Process content and analysis in a single transaction."""
    try:
        with db_manager.get_session() as session:
            # Create content record
            content_repo = ContentRepository(session)
            content_result = content_repo.create(content_data, tenant, workspace)

            if not content_result.success:
                return content_result

            content_id = content_result.data['id']

            # Generate and store analysis
            analysis_result = analyze_service.process(
                content=content_data['content'],
                tenant=tenant,
                workspace=workspace
            )

            if not analysis_result.success:
                return analysis_result

            # Store analysis record
            analysis_repo = ContentAnalysisRepository(session)
            analysis_repo.create(
                content_id=content_id,
                analysis_type='comprehensive',
                analysis_result=analysis_result.data,
                tenant=tenant,
                workspace=workspace
            )

            # Generate and store vector embeddings
            vector_data = embedding_service.generate_embeddings(
                content_data['content'],
                tenant=tenant,
                workspace=workspace
            )

            vector_repo = MemoryVectorRepository(session)
            vector_repo.store_vector(
                content_id=content_id,
                vector_data=vector_data,
                vector_type='content',
                tenant=tenant,
                workspace=workspace
            )

            # Transaction commits automatically here
            return StepResult.ok(data={
                'content_id': content_id,
                'analysis_stored': True,
                'vectors_stored': True
            })

    except Exception as e:
        # Automatic rollback on exception
        return StepResult.fail(f"Transaction failed: {str(e)}")
```
