"""Compatibility shim forwarding to the modular OpenRouter service package."""
from __future__ import annotations

import os as _os
import time as _time
from platform.http.http_utils import resilient_post as _default_resilient_post


__path__ = [_os.path.join(_os.path.dirname(__file__), 'openrouter_service')]
from .openrouter_service.service import OpenRouterService


resilient_post = _default_resilient_post
time = _time
_semantic_cache_get = None
__all__ = ['OpenRouterService', '_semantic_cache_get', 'resilient_post', 'time']
legacy_source = '\n\n# Optional dependencies (graceful degradation if unavailable)\ntry:  # pragma: no cover - optional distributed cache\n    from core.cache.enhanced_redis_cache import DistributedLLMCache\nexcept Exception:  # pragma: no cover\n    DistributedLLMCache = None\n\nfrom core.http_utils import (\n    REQUEST_TIMEOUT_SECONDS,\n    http_request_with_retry,\n    is_retry_enabled,\n    resilient_post,\n)\nfrom core.learning_engine import LearningEngine\nfrom core.secure_config import get_config\n\ntry:\n    # Prefer src.core.settings import path to align with tests that patch via src.core.settings\n    from src.core.settings import get_settings  # type: ignore\nexcept Exception:\n    try:\n        from ultimate_discord_intelligence_bot.settings import Settings  # type: ignore\n    except Exception:  # pragma: no cover - fallback when pydantic/settings unavailable\n        # Create fallback function to avoid redefinition error\n        def _get_settings_fallback() -> StepResult:  # minimal shim (return object with expected attrs)\n            class _S:  # minimal shim for tests\n                reward_cost_weight = 0.5\n                reward_latency_weight = 0.5\n                reward_latency_ms_window = 2000\n                openrouter_referer = None\n                openrouter_title = None\n                enable_vllm_local = False\n                local_llm_url = None\n\n            return _S()\n\n        get_settings = _get_settings_fallback  # type: ignore[assignment]\n\n\nfrom obs import metrics\n\ntry:\n    from obs.enhanced_langsmith_integration import trace_llm_call as _trace_llm_call\n\n    def trace_llm_call(*args: Any, **kwargs: Any) -> StepResult:\n        _trace_llm_call(*args, **kwargs)\nexcept Exception:  # pragma: no cover\n\n    def trace_llm_call(*args: Any, **kwargs: Any) -> StepResult:\n        return None\n\n\nfrom ultimate_discord_intelligence_bot.tenancy.context import TenantContext\nfrom ultimate_discord_intelligence_bot.tenancy.registry import TenantRegistry\n\nfrom .cache import RedisLLMCache, make_key\nfrom .logging_utils import AnalyticsStore\nfrom .openrouter_helpers import (\n    choose_model_from_map as _choose_model_from_map_helper,\n)\nfrom .openrouter_helpers import (\n    ctx_or_fallback as _ctx_or_fallback_helper,\n)\nfrom .openrouter_helpers import (\n    deep_merge as _deep_merge_helper,\n)\nfrom .openrouter_helpers import (\n    update_shadow_hit_ratio as _update_shadow_hit_ratio_helper,\n)\nfrom .prompt_engine import PromptEngine\nfrom .request_budget import current_request_tracker as _crt\nfrom .token_meter import TokenMeter\nfrom ultimate_discord_intelligence_bot.step_result import StepResult\n\n# Optional semantic cache (guarded by feature flag in settings)\n_semantic_cache_get = None\n_semantic_cache_factory_err: str | None = None\ntry:\n    from core.cache.semantic_cache import get_semantic_cache as _get_semantic_cache\n\n    _semantic_cache_get = _get_semantic_cache\nexcept Exception as _sc_exc:  # pragma: no cover - optional feature\n    _semantic_cache_factory_err = str(_sc_exc)\n\n# Optional local vLLM adapter (placed after all imports to satisfy E402). Define aliases once.\ntry:  # pragma: no cover - optional local vLLM adapter\n    from core.vllm_service import is_vllm_available as _is_vllm_available  # type: ignore[unused-ignore]\n    from core.vllm_service import vLLMOpenRouterAdapter as _VLLMAdapterCtor  # type: ignore[unused-ignore]\nexcept Exception:  # pragma: no cover\n    _is_vllm_available = None\n    _VLLMAdapterCtor = None\nvllm_adapter_ctor: Any | None = _VLLMAdapterCtor\n\n\ndef _has_vllm() -> StepResult:\n    """Return True if local vLLM adapter is importable and available."""\n    try:\n        if _is_vllm_available is None:\n            return False\n        return bool(_is_vllm_available())\n    except Exception:\n        return False\n\n\nlog = logging.getLogger(__name__)\n\n\nclass OpenRouterService:\n    """Route prompts to the best model and provider available."""\n\n    def __init__(  # noqa: PLR0913, PLR0912 - wide configuration surface & branching for config selection is acceptable here\n        self,\n        models_map: dict[str, list[str]] | None = None,\n        learning_engine: LearningEngine | None = None,\n        api_key: str | None = None,\n        provider_opts: dict[str, Any] | None = None,\n        logger: AnalyticsStore | None = None,\n        token_meter: TokenMeter | None = None,\n        cache: Any | None = None,\n        tenant_registry: TenantRegistry | None = None,\n    ) -> StepResult:\n        """Initialise the router.\n\n        Args:\n            models_map: Optional mapping of task types to model lists.\n            learning_engine: Bandit-based learner for model selection.\n            api_key: OpenRouter API key; when absent the service operates offline.\n            provider_opts: Default provider routing preferences applied to all\n                requests. A deep copy is stored to avoid accidental mutation of\n                caller data. Nested dictionaries are merged with call-level\n                overrides when routing.\n        """\n        # Environment variables allow deployment-time model overrides without\n        # changing source. ``OPENROUTER_GENERAL_MODEL`` sets the default model\n        # for unspecified task types while ``OPENROUTER_ANALYSIS_MODEL`` can\n        # specialise the analysis route.\n        config = get_config()\n        env_general = config.get_setting("openrouter_general_model")\n        env_analysis = config.get_setting("openrouter_analysis_model")\n        default_map = {\n            "general": [env_general or "openai/gpt-4o-mini"],\n            "analysis": [env_analysis or env_general or "openai/gpt-4o-mini"],\n        }\n        if models_map:\n            default_map.update(models_map)\n        self.models_map = default_map\n        self.learning = learning_engine or LearningEngine()\n        # API key may be absent in offline/dev mode. Treat an explicit argument\n        # (including empty string) as authoritative so tests can force offline\n        # mode by passing api_key="" rather than falling back to secure config.\n        self.api_key: str | None\n        if api_key is not None:\n            # Explicit argument provided (including empty string). Empty string -> force offline.\n            self.api_key = api_key or None\n        else:\n            # Caller omitted api_key; use raw config attribute directly (avoid raising) and treat blank/placeholder as absent.\n            cand = getattr(config, "openrouter_api_key", None)\n            # If the secure_config still has a cached key but the environment variable was removed\n            # (common in tests using monkeypatch.delenv), treat as absent so we enter offline mode.\n            if cand and _os.getenv("OPENROUTER_API_KEY") is None:\n                cand = None\n            if isinstance(cand, str) and not cand.strip():  # blank\n                cand = None\n                # Some test fixtures may inject a fake looking key to ensure code paths don\'t 400; if they want offline they unset env.\n                # Keep as-is otherwise.\n            self.api_key = cand\n        # Convenience flag for tests to introspect offline mode quickly.\n        # Preserve initial offline determination; later route calls should not override this.\n        self.offline_mode = not bool(self.api_key)\n        self.prompt_engine = PromptEngine()\n        self.token_meter = token_meter or TokenMeter()\n        # Choose cache implementation - prefer enhanced Redis cache for performance\n        self.cache = None\n        if cache is not None:\n            self.cache = cache\n        else:\n            try:\n                cfg = get_config()\n                if getattr(cfg, "enable_cache_global", True) and getattr(cfg, "rate_limit_redis_url", None):\n                    if DistributedLLMCache is not None:\n                        try:\n                            # Resolve tenant context for cache namespacing (allows fallback in non-strict mode)\n                            ctx_cache = OpenRouterService._ctx_or_fallback("openrouter_service")\n                            tenant_id = (getattr(ctx_cache, "tenant_id", None) or "default") if ctx_cache else "default"\n                            workspace_id = (getattr(ctx_cache, "workspace_id", None) or "main") if ctx_cache else "main"\n                            CacheCtor: Any = DistributedLLMCache\n                            self.cache = CacheCtor(\n                                url=str(cfg.rate_limit_redis_url),\n                                ttl=int(getattr(cfg, "cache_ttl_llm", 3600)),\n                                tenant=tenant_id,\n                                workspace=workspace_id,\n                            )\n                        except Exception as dist_exc:  # pragma: no cover - best effort cache upgrade\n                            log.debug("Falling back to RedisLLMCache (Distributed init failed): %s", dist_exc)\n                            self.cache = None\n                    if self.cache is None:\n                        try:\n                            self.cache = RedisLLMCache(\n                                url=str(cfg.rate_limit_redis_url),\n                                ttl=int(getattr(cfg, "cache_ttl_llm", 3600)),\n                            )\n                        except Exception as redis_exc:  # pragma: no cover\n                            log.debug("RedisLLMCache initialisation failed, disabling cache: %s", redis_exc)\n                            self.cache = None\n            except Exception as cache_root_exc:  # pragma: no cover\n                log.debug("Cache configuration unavailable, disabling cache: %s", cache_root_exc)\n                self.cache = None\n        # Deep copy to avoid mutating caller-supplied dictionaries when merging\n        self.provider_opts = copy.deepcopy(provider_opts or {})\n        self.logger = logger\n        self.tenant_registry = tenant_registry\n        # Semantic cache instance (only if enabled via settings or env fallback)\n        try:\n            settings = Settings()\n            enabled_sem = bool(getattr(settings, "enable_semantic_cache", False))\n            if not enabled_sem:\n                raw = (_os.getenv("ENABLE_SEMANTIC_CACHE") or "").lower()\n                enabled_sem = raw in ("1", "true", "yes", "on")\n            if enabled_sem and _semantic_cache_get:\n                self.semantic_cache = _semantic_cache_get()\n            else:\n                self.semantic_cache = None\n\n            # Check for shadow mode (separate from production semantic cache)\n            enabled_shadow = bool(getattr(settings, "enable_semantic_cache_shadow", False))\n            if not enabled_shadow:\n                raw_shadow = (_os.getenv("ENABLE_SEMANTIC_CACHE_SHADOW") or "").lower()\n                enabled_shadow = raw_shadow in ("1", "true", "yes", "on")\n\n            # In shadow mode, we may create a semantic cache instance for tracking, but tests expect\n            # semantic_cache to remain None when ENABLE_SEMANTIC_CACHE is disabled. Track shadow mode\n            # via a boolean and access the factory lazily when routing (not stored on the service).\n            self.semantic_cache_shadow_mode = enabled_shadow\n            if enabled_shadow and not enabled_sem and self.semantic_cache is None:\n                # Shadow mode should have an instance for tracking, but not used for responses\n                self.semantic_cache = _semantic_cache_get() if _semantic_cache_get else None\n            # Optional promotion of shadow hits to production usage when similarity exceeds a threshold\n            promote_flag = bool(getattr(settings, "enable_semantic_cache_promotion", False))\n            if not promote_flag:\n                raw_promote = (_os.getenv("ENABLE_SEMANTIC_CACHE_PROMOTION") or "").lower()\n                promote_flag = raw_promote in ("1", "true", "yes", "on")\n            self.semantic_cache_promotion_enabled = promote_flag\n            # Threshold (default 0.9) can be overridden via settings or env\n            thr = getattr(settings, "semantic_cache_promotion_threshold", None)\n            if thr is None:\n                env_thr = _os.getenv("SEMANTIC_CACHE_PROMOTION_THRESHOLD")\n                try:\n                    thr = float(env_thr) if env_thr is not None and env_thr.strip() != "" else 0.9\n                except Exception:\n                    thr = 0.9\n            try:\n                self.semantic_cache_promotion_threshold = float(thr)\n            except Exception:\n                self.semantic_cache_promotion_threshold = 0.9\n        except Exception:\n            self.semantic_cache = None\n            self.semantic_cache_shadow_mode = False\n            self.semantic_cache_promotion_enabled = False\n            self.semantic_cache_promotion_threshold = 0.9\n\n    # --- Tenancy helpers -------------------------------------------------\n    @staticmethod\n    def _ctx_or_fallback(component: str) -> StepResult:\n        """Delegate to shared helper for tenancy fallback logic."""\n        return _ctx_or_fallback_helper(component)\n\n    @staticmethod\n    def _deep_merge(base: dict[str, Any], overrides: Mapping[str, Any]) -> StepResult:\n        """Delegate to shared helper for deep merge logic."""\n        return _deep_merge_helper(base, overrides)\n\n    def _update_shadow_hit_ratio(self, labels: dict[str, str], is_hit: bool) -> StepResult:\n        """Delegate to shared helper for shadow hit ratio metric updates."""\n        _update_shadow_hit_ratio_helper(labels, is_hit)\n\n    def _choose_model_from_map(self, task_type: str, models_map: dict[str, list[str]]) -> StepResult:\n        """Delegate to shared helper for model selection from map (keeps signature)."""\n        return _choose_model_from_map_helper(task_type, models_map, self.learning)\n\n    def route(  # noqa: PLR0912, PLR0913, PLR0915, C901 - complex orchestrator; slated for helper extraction\n        self,\n        prompt: str,\n        task_type: str = "general",\n        model: str | None = None,\n        provider_opts: dict[str, Any] | None = None,\n    ) -> StepResult:\n        """Route a prompt with provider preferences and budget enforcement.\n\n        Note: This function coordinates several concerns (tenancy, pricing,\n        budgeting, caching, offline/network execution, metrics, tracing). A\n        follow-up refactor will extract helpers to reduce complexity while\n        preserving behavior.\n        """\n        # Resolve tenant context early (strict mode can raise; non-strict records fallback once per call)\n        ctx_effective = OpenRouterService._ctx_or_fallback("openrouter_service")\n\n        # Ensure metrics are attributed to the effective tenant/workspace even when no\n        # TenantContext was pre-set by the caller. We avoid mutating thread-local state\n        # and instead provide a local label factory for metric calls.\n        def _labels() -> StepResult:\n            if ctx_effective is not None:\n                return {\n                    "tenant": getattr(ctx_effective, "tenant_id", "unknown"),\n                    "workspace": getattr(ctx_effective, "workspace_id", "unknown"),\n                }\n            return metrics.label_ctx()\n\n        # Effective model map (include tenant overrides)\n        effective_models = copy.deepcopy(self.models_map)\n        if self.tenant_registry:\n            ctx = ctx_effective\n            if ctx:\n                overrides = self.tenant_registry.get_model_overrides(ctx)\n                if overrides:\n                    default_override = overrides.get("default")\n                    if default_override:\n                        effective_models.setdefault("general", [default_override])\n                        # If no explicit analysis override is provided, align analysis with the\n                        # default override so budgeting tests that specify only a default model\n                        # still apply that model to analysis tasks (mirrors intuitive expectation\n                        # that \'default\' applies to all unspecified task types).\n                        if "analysis" not in overrides:\n                            effective_models["analysis"] = [default_override]\n                    for k, v in overrides.items():\n                        if k != "default":\n                            effective_models[k] = [v]\n        # Do not override original offline choice; allow env var to enable only if we were not explicitly offline\n        if not self.api_key and not self.offline_mode:\n            env_key = _os.getenv("OPENROUTER_API_KEY")\n            if env_key:\n                self.api_key = env_key\n                self.offline_mode = False\n        effective_offline = self.offline_mode or not self.api_key\n        chosen = model or self._choose_model_from_map(task_type, effective_models)\n        # Provider preferences\n        provider: dict[str, Any] = {}\n        if self.tenant_registry:\n            ctx = ctx_effective\n            if ctx:\n                prefs = self.tenant_registry.get_provider_preferences(ctx)\n                if prefs:\n                    provider = {"order": prefs}\n        provider = self._deep_merge(provider, copy.deepcopy(self.provider_opts)) if self.provider_opts else provider\n        if provider_opts:\n            provider = self._deep_merge(provider, provider_opts)\n        provider_family = "unknown"\n        try:\n            order = provider.get("order") if isinstance(provider, dict) else None\n            if isinstance(order, list) and order:\n                provider_family = str(order[0])\n        except Exception:\n            provider_family = "unknown"\n\n        # ------------------------------------------------------------------\n        # Estimate cost & enforce budgeting constraints (cumulative + per-call)\n        # ------------------------------------------------------------------\n        tokens_in = self.prompt_engine.count_tokens(prompt, chosen)\n        effective_prices = dict(self.token_meter.model_prices)\n        if self.tenant_registry:\n            ctx = ctx_effective\n            if ctx:\n                effective_prices.update(self.tenant_registry.get_pricing_map(ctx))\n        projected_cost = self.token_meter.estimate_cost(tokens_in, chosen, prices=effective_prices)\n        affordable = self.token_meter.affordable_model(\n            tokens_in,\n            effective_models.get(task_type, effective_models.get("general", [])),\n            prices=effective_prices,\n        )\n        # Cumulative tracker\n        tracker = _crt()\n        if tracker and not tracker.can_charge(projected_cost, task_type):\n            # try cheaper option\n            if affordable and affordable != chosen:\n                alt_tokens = self.prompt_engine.count_tokens(prompt, affordable)\n                alt_cost = self.token_meter.estimate_cost(alt_tokens, affordable, prices=effective_prices)\n                if tracker.can_charge(alt_cost, task_type):\n                    chosen = affordable\n                    tokens_in = alt_tokens\n                    projected_cost = alt_cost\n                else:\n                    metrics.LLM_BUDGET_REJECTIONS.labels(**_labels(), task=task_type, provider=provider_family).inc()\n                    return {\n                        "status": "error",\n                        "error": "cumulative cost exceeds limit",\n                        "model": chosen,\n                        "tokens": tokens_in,\n                        "provider": provider,\n                    }\n            else:\n                metrics.LLM_BUDGET_REJECTIONS.labels(**_labels(), task=task_type, provider=provider_family).inc()\n                return {\n                    "status": "error",\n                    "error": "cumulative cost exceeds limit",\n                    "model": chosen,\n                    "tokens": tokens_in,\n                    "provider": provider,\n                }\n\n        # Per-request limit overrides\n        tenant_max: float | None = None\n        if self.tenant_registry:\n            ctx = ctx_effective\n            if ctx:\n                per_task_limit = self.tenant_registry.get_per_request_limit(ctx, task_type)\n                if per_task_limit is not None:\n                    tenant_max = per_task_limit\n                else:\n                    bcfg = self.tenant_registry.get_budget_config(ctx.tenant_id)\n                    if isinstance(bcfg, dict):\n                        limits = bcfg.get("limits") if isinstance(bcfg.get("limits"), dict) else None\n                        if limits and "max_per_request" in limits:\n                            try:\n                                tenant_max = float(limits["max_per_request"])\n                            except Exception:\n                                tenant_max = None\n                        if tenant_max is None and "max_per_request" in bcfg:\n                            try:\n                                raw_val = bcfg.get("max_per_request")\n                                tenant_max = float(raw_val) if raw_val is not None else None\n                            except Exception:\n                                tenant_max = None\n        effective_max = (\n            self.token_meter.max_cost_per_request if self.token_meter.max_cost_per_request is not None else float("inf")\n        )\n        if tenant_max is not None:\n            effective_max = min(effective_max, tenant_max)\n        # Strict per-request limit enforcement: if current model exceeds limit attempt\n        # an affordable fallback ONCE; if still over, emit error immediately.\n        if projected_cost > effective_max:\n            if affordable and affordable != chosen:\n                alt_tokens = self.prompt_engine.count_tokens(prompt, affordable)\n                alt_cost = self.token_meter.estimate_cost(alt_tokens, affordable, prices=effective_prices)\n                if alt_cost <= effective_max:\n                    chosen = affordable\n                    tokens_in = alt_tokens\n                    projected_cost = alt_cost\n                else:\n                    metrics.LLM_BUDGET_REJECTIONS.labels(**_labels(), task=task_type, provider=provider_family).inc()\n                    return {\n                        "status": "error",\n                        "error": "projected cost exceeds limit",\n                        "model": chosen,\n                        "tokens": tokens_in,\n                        "provider": provider,\n                    }\n            else:\n                metrics.LLM_BUDGET_REJECTIONS.labels(**_labels(), task=task_type, provider=provider_family).inc()\n                return {\n                    "status": "error",\n                    "error": "projected cost exceeds limit",\n                    "model": chosen,\n                    "tokens": tokens_in,\n                    "provider": provider,\n                }\n\n        # Cache lookup (semantic first, then traditional)\n        cache_key = None\n        # Tenant namespace for scoping\n        ns = None\n        if ctx_effective:\n            try:\n                ns = f"{getattr(ctx_effective, \'tenant_id\', \'unknown\')}:{getattr(ctx_effective, \'workspace_id\', \'unknown\')}"\n            except Exception:\n                ns = None\n        # Semantic cache\n        if self.semantic_cache is not None:\n            try:\n                # Run coroutine in a background thread using asyncio.run so it works\n                # regardless of whether a loop is already running in this thread.\n                import asyncio as _asyncio\n                import threading as _threading\n\n                _holder: dict[str, Any] = {}\n                log.debug(\n                    "semantic_cache_get attempting (ns=%s, model=%s, type=%s)", ns, chosen, type(self.semantic_cache)\n                )\n\n                sc = self.semantic_cache\n\n                def _runner() -> StepResult:\n                    try:\n                        if sc is not None:\n                            _holder["result"] = _asyncio.run(sc.get(prompt, chosen, namespace=ns))\n                    except Exception as e:  # pragma: no cover - defensive\n                        _holder["error"] = e\n\n                t = _threading.Thread(target=_runner, daemon=True)\n                t.start()\n                t.join()\n                if "error" not in _holder:\n                    sem_res = _holder.get("result")\n                    if sem_res is not None:\n                        log.debug("semantic_cache_get HIT for model=%s ns=%s", chosen, ns)\n\n                        # In shadow mode, optionally promote to production if similarity exceeds threshold\n                        if self.semantic_cache_shadow_mode:\n                            try:\n                                metrics.SEMANTIC_CACHE_SHADOW_HITS.labels(**_labels(), model=chosen).inc()\n                                self._update_shadow_hit_ratio(_labels(), is_hit=True)\n                            except Exception:  # pragma: no cover\n                                pass\n                            # Decide on promotion\n                            try:\n                                sim_val = float(sem_res.get("similarity", 0.0)) if isinstance(sem_res, dict) else 0.0\n                            except Exception:\n                                sim_val = 0.0\n                            promote = bool(\n                                getattr(self, "semantic_cache_promotion_enabled", False)\n                            ) and sim_val >= float(getattr(self, "semantic_cache_promotion_threshold", 0.9))\n                            if promote:\n                                # Use as a real cache hit\n                                result = dict(sem_res) if isinstance(sem_res, dict) else {"response": str(sem_res)}\n                                result["cached"] = True\n                                result["cache_type"] = "semantic"\n                                # Record similarity bucket\n                                try:\n                                    if sim_val >= 0.9:\n                                        bucket = ">=0.9"\n                                    elif sim_val >= 0.75:\n                                        bucket = "0.75-0.9"\n                                    else:\n                                        bucket = "<0.75"\n                                    metrics.SEMANTIC_CACHE_SIMILARITY.labels(**_labels(), bucket=bucket).observe(\n                                        sim_val\n                                    )\n                                except Exception:  # pragma: no cover\n                                    pass\n                                try:\n                                    # Record a promotion from shadow semantic cache into production usage\n                                    # Low-cardinality labels: tenant/workspace from _labels(), cache_name fixed\n                                    metrics.CACHE_PROMOTIONS.labels(**_labels(), cache_name="semantic").inc()\n                                    metrics.LLM_CACHE_HITS.labels(\n                                        **_labels(), model=chosen, provider=provider_family\n                                    ).inc()\n                                    metrics.SEMANTIC_CACHE_PREFETCH_USED.labels(**_labels()).inc()\n                                except Exception:  # pragma: no cover\n                                    pass\n                                return result\n                            else:\n                                log.debug(\n                                    "semantic_cache SHADOW HIT (no promotion) tracked for model=%s ns=%s sim=%.3f",\n                                    chosen,\n                                    ns,\n                                    sim_val,\n                                )\n                        else:\n                            # Production mode - return the cached result\n                            result = dict(sem_res)\n                            result["cached"] = True\n                            result["cache_type"] = "semantic"\n                            # Record similarity (if provided) into histogram buckets for observability\n                            try:\n                                sim_val = float(result.get("similarity", 0.0))\n                                # Bucket label keeps low cardinality; refine later if distribution warrants.\n                                if sim_val >= 0.9:\n                                    bucket = ">=0.9"\n                                elif sim_val >= 0.75:\n                                    bucket = "0.75-0.9"\n                                else:\n                                    bucket = "<0.75"\n                                metrics.SEMANTIC_CACHE_SIMILARITY.labels(**_labels(), bucket=bucket).observe(sim_val)\n                            except Exception:  # pragma: no cover - best effort metrics\n                                pass\n                            # Prefetch USED: a prior miss would have issued a prefetch for this prompt/model.\n                            try:\n                                metrics.SEMANTIC_CACHE_PREFETCH_USED.labels(**_labels()).inc()\n                            except Exception:  # pragma: no cover\n                                pass\n                            metrics.LLM_CACHE_HITS.labels(**_labels(), model=chosen, provider=provider_family).inc()\n                            return result\n                    else:\n                        log.debug("semantic_cache_get MISS for model=%s ns=%s", chosen, ns)\n\n                        # Track miss in shadow mode or production\n                        if self.semantic_cache_shadow_mode:\n                            try:\n                                metrics.SEMANTIC_CACHE_SHADOW_MISSES.labels(**_labels(), model=chosen).inc()\n                                self._update_shadow_hit_ratio(_labels(), is_hit=False)\n                            except Exception:\n                                pass\n                        else:\n                            try:\n                                metrics.LLM_CACHE_MISSES.labels(\n                                    **_labels(), model=chosen, provider=provider_family\n                                ).inc()\n                                # Miss triggers a prefetch issuance (we will store response post-call via set())\n                                metrics.SEMANTIC_CACHE_PREFETCH_ISSUED.labels(**_labels()).inc()\n                            except Exception:\n                                pass\n            except Exception:\n                # Conservative: ignore semantic cache errors\n                pass\n        if self.cache:\n            norm_prompt = prompt\n\n            def _sig(obj: Any) -> StepResult:\n                if isinstance(obj, dict):\n                    return "{" + ",".join(f"{k}:{_sig(obj[k])}" for k in sorted(obj)) + "}"\n                if isinstance(obj, list):\n                    return "[" + ",".join(_sig(x) for x in obj) + "]"\n                return str(obj)\n\n            provider_sig = _sig(provider) if provider else "{}"\n            cache_key = make_key(f"{norm_prompt}|provider={provider_sig}", chosen)\n            cached = self.cache.get(cache_key)\n            if cached:\n                result = dict(cached)\n                result["cached"] = True\n                metrics.LLM_CACHE_HITS.labels(\n                    **_labels(), model=result.get("model", chosen), provider=provider_family\n                ).inc()\n                return result\n\n        start = time.perf_counter()\n        if effective_offline:  # offline path\n            # Offline deterministic echo (historically uppercased for test stability)\n            response = prompt.upper()\n            latency_ms = (time.perf_counter() - start) * 1000\n            tokens_out = self.prompt_engine.count_tokens(response, chosen)\n            settings = Settings()\n            rl: dict[str, float] = {}\n            if self.tenant_registry:\n                ctx_t = ctx_effective\n                rl = self.tenant_registry.get_rl_overrides(ctx_t) if ctx_t else {}\n            w_cost = float(rl.get("reward_cost_weight", getattr(settings, "reward_cost_weight", 0.5) or 0.5))\n            w_lat = float(rl.get("reward_latency_weight", getattr(settings, "reward_latency_weight", 0.5) or 0.5))\n            if w_cost == 0.0 and w_lat == 0.0:\n                w_cost = w_lat = 0.5\n            norm = w_cost + w_lat\n            w_cost /= norm\n            w_lat /= norm\n            cost_norm = 0.0\n            if projected_cost > 0:\n                # When explicit RL overrides are provided we normalise cost against the\n                # observed projected cost itself (treating it as 100%) so a cost weight of 1.0\n                # drives reward close to 0 as tests expect. Otherwise retain scaling relative\n                # to the effective max budget for smoother shaping.\n                if rl.get("reward_cost_weight") is not None or rl.get("reward_latency_weight") is not None:\n                    denom = projected_cost\n                else:\n                    denom = effective_max if effective_max and effective_max != float("inf") else projected_cost\n                cost_norm = min(1.0, projected_cost / max(denom, 1e-9))\n            lat_window = float(\n                rl.get("reward_latency_ms_window", getattr(settings, "reward_latency_ms_window", 2000) or 2000)\n            )\n            lat_window = max(1.0, lat_window)\n            lat_norm = min(1.0, latency_ms / lat_window)\n            reward = max(0.0, 1.0 - w_cost * cost_norm - w_lat * lat_norm)\n            self.learning.update(task_type, chosen, reward=reward)\n            metrics.LLM_MODEL_SELECTED.labels(**_labels(), task=task_type, model=chosen, provider=provider_family).inc()\n            metrics.LLM_ESTIMATED_COST.labels(**_labels(), model=chosen, provider=provider_family).observe(\n                projected_cost\n            )\n            metrics.LLM_LATENCY.labels(**_labels()).observe(latency_ms)\n            if self.logger:\n                self.logger.log_llm_call(\n                    task_type,\n                    chosen,\n                    str(provider),\n                    tokens_in,\n                    tokens_out,\n                    float(projected_cost),\n                    latency_ms,\n                    None,\n                    True,\n                    None,\n                )\n            result = {\n                "status": "success",\n                "model": chosen,\n                "response": response,\n                "tokens": tokens_in,\n                "provider": provider,\n            }\n            trace_llm_call(\n                name=f"openrouter_{task_type}",\n                prompt=prompt,\n                response=response,\n                model=chosen,\n                metadata={"provider": provider, "task_type": task_type, "offline": True},\n                latency_ms=latency_ms,\n                token_usage={\n                    "input_tokens": tokens_in,\n                    "output_tokens": tokens_out,\n                    "total_tokens": tokens_in + tokens_out,\n                },\n                cost=projected_cost,\n            )\n            # Persist caches\n            if self.semantic_cache is not None:\n                try:\n                    import asyncio as _asyncio\n                    import threading as _threading\n\n                    sc = self.semantic_cache\n\n                    def _runner_set() -> StepResult:\n                        try:\n                            if sc is not None:\n                                _asyncio.run(sc.set(prompt, chosen, result, namespace=ns))\n                        except Exception:\n                            pass\n\n                    t_set = _threading.Thread(target=_runner_set, daemon=True)\n                    t_set.start()\n                    t_set.join()\n                    log.debug("semantic_cache_set completed for model=%s ns=%s", chosen, ns)\n                except Exception:\n                    pass\n            if self.cache and cache_key:\n                self.cache.set(cache_key, result)\n            result["cached"] = False\n            if tracker:\n                try:\n                    tracker.charge(projected_cost, task_type)\n                except Exception as charge_exc:  # pragma: no cover - defensive accounting\n                    log.debug(\n                        "request budget charge failed (offline path) task=%s model=%s err=%s",\n                        task_type,\n                        chosen,\n                        charge_exc,\n                    )\n            return result\n        # Network path\n        try:  # pragma: no cover\n            payload: dict[str, Any] = {"model": chosen, "messages": [{"role": "user", "content": prompt}]}\n            if provider:\n                payload["provider"] = provider\n            settings = Settings()\n            if (\n                chosen.startswith("local/")\n                and vllm_adapter_ctor is not None\n                and _has_vllm()\n                and getattr(settings, "enable_vllm_local", False)\n            ):\n                try:  # pragma: no cover - only when local inference available\n                    AdapterType: Any = vllm_adapter_ctor\n                    adapter = AdapterType()\n                    vllm_result = adapter.route_to_local_model(prompt, chosen, task_type)\n                    if vllm_result.get("status") == "success":\n                        latency_ms = (time.perf_counter() - start) * 1000\n                        trace_llm_call(\n                            name=f"vllm_{task_type}",\n                            prompt=prompt,\n                            response=vllm_result["response"],\n                            model=chosen,\n                            metadata={"provider": "vllm", "task_type": task_type, "local_inference": True},\n                            latency_ms=latency_ms,\n                            token_usage={\n                                "input_tokens": tokens_in,\n                                "output_tokens": vllm_result.get("tokens", 0),\n                                "total_tokens": tokens_in + vllm_result.get("tokens", 0),\n                            },\n                            cost=0.0,\n                        )\n                        if tracker:\n                            try:\n                                tracker.charge(projected_cost, task_type)\n                            except Exception as charge_exc:  # pragma: no cover\n                                log.debug(\n                                    "request budget charge failed (vLLM path) task=%s model=%s err=%s",\n                                    task_type,\n                                    chosen,\n                                    charge_exc,\n                                )\n                        return vllm_result\n                except Exception as e:  # pragma: no cover\n                    log.warning("vLLM local inference failed: %s, falling back to HTTP", e)\n                if getattr(settings, "local_llm_url", None):\n                    local_model = chosen.split("/", 1)[1] if "/" in chosen else chosen\n                    local_payload = {"model": local_model, "messages": payload["messages"]}\n                    local_url = str(getattr(settings, "local_llm_url", "")).rstrip("/") + "/v1/chat/completions"\n                    resp = resilient_post(\n                        local_url, json_payload=local_payload, timeout_seconds=REQUEST_TIMEOUT_SECONDS\n                    )\n                    if resp is None or getattr(resp, "status_code", 200) >= 400:  # noqa: PLR2004\n                        raise RuntimeError(f"local_llm_error status={resp.status_code}")\n                    data = resp.json() if resp is not None else {}\n                    message = data.get("choices", [{}])[0].get("message", {}).get("content", "")\n                    latency_ms = (time.perf_counter() - start) * 1000\n                    tokens_out = self.prompt_engine.count_tokens(message, local_model)\n                    result_local = {\n                        "status": "success",\n                        "model": chosen,\n                        "response": message,\n                        "tokens": tokens_in,\n                        "provider": {"order": ["local"]},\n                    }\n                    if tracker:\n                        try:\n                            tracker.charge(projected_cost, task_type)\n                        except Exception as charge_exc:  # pragma: no cover\n                            log.debug(\n                                "request budget charge failed (local_llm path) task=%s model=%s err=%s",\n                                task_type,\n                                chosen,\n                                charge_exc,\n                            )\n                    return result_local\n            url = "https://openrouter.ai/api/v1/chat/completions"\n            api_key = self.api_key or ""\n            headers = {\n                "Authorization": f"Bearer {api_key}",\n                "Accept": "application/json",\n                "Content-Type": "application/json",\n            }\n            ref = getattr(settings, "openrouter_referer", None) or _os.getenv("OPENROUTER_REFERER")\n            if ref:\n                ref = str(ref)\n                headers["Referer"] = ref\n                headers["HTTP-Referer"] = ref\n            title = getattr(settings, "openrouter_title", None) or _os.getenv("OPENROUTER_TITLE")\n            if title:\n                headers["X-Title"] = str(title)\n            resp = None\n            if is_retry_enabled():\n                try:\n                    resp = http_request_with_retry(\n                        "POST",\n                        url,\n                        request_callable=lambda u, **_: resilient_post(\n                            u, headers=headers, json_payload=payload, timeout_seconds=REQUEST_TIMEOUT_SECONDS\n                        ),\n                        max_attempts=3,\n                    )\n                except Exception as retry_exc:\n                    # Give-up path: record error status and surface message\n                    latency_ms = (time.perf_counter() - start) * 1000\n                    if self.logger:\n                        try:\n                            self.logger.log_llm_call(\n                                task_type,\n                                chosen,\n                                str(provider),\n                                tokens_in,\n                                0,\n                                float(projected_cost),\n                                latency_ms,\n                                None,\n                                False,\n                                str(retry_exc),\n                            )\n                        except Exception as log_exc:  # pragma: no cover\n                            log.debug("logger failure in retry give-up: %s", log_exc)\n                    return {\n                        "status": "error",\n                        "error": str(retry_exc),\n                        "model": chosen,\n                        "tokens": tokens_in,\n                        "provider": provider,\n                    }\n            else:\n                resp = resilient_post(\n                    url, headers=headers, json_payload=payload, timeout_seconds=REQUEST_TIMEOUT_SECONDS\n                )\n            if resp is None or getattr(resp, "status_code", 200) >= 400:  # noqa: PLR2004\n                code = getattr(resp, "status_code", "unknown")\n                raise RuntimeError(f"openrouter_error status={code}")\n            data = resp.json() if resp is not None else {}\n            message = data.get("choices", [{}])[0].get("message", {}).get("content", "")\n            latency_ms = (time.perf_counter() - start) * 1000\n            tokens_out = self.prompt_engine.count_tokens(message, chosen)\n            settings = Settings()\n            rl = {}\n            if self.tenant_registry:\n                ctx_t = ctx_effective\n                rl = self.tenant_registry.get_rl_overrides(ctx_t) if ctx_t else {}\n            w_cost = float(rl.get("reward_cost_weight", getattr(settings, "reward_cost_weight", 0.5) or 0.5))\n            w_lat = float(rl.get("reward_latency_weight", getattr(settings, "reward_latency_weight", 0.5) or 0.5))\n            if w_cost == 0.0 and w_lat == 0.0:\n                w_cost = w_lat = 0.5\n            norm = w_cost + w_lat\n            w_cost /= norm\n            w_lat /= norm\n            cost_norm = 0.0\n            if projected_cost > 0:\n                if rl.get("reward_cost_weight") is not None or rl.get("reward_latency_weight") is not None:\n                    denom = projected_cost\n                else:\n                    denom = effective_max if effective_max and effective_max != float("inf") else projected_cost\n                cost_norm = min(1.0, projected_cost / max(denom, 1e-9))\n            lat_window = float(\n                rl.get("reward_latency_ms_window", getattr(settings, "reward_latency_ms_window", 2000) or 2000)\n            )\n            lat_window = max(1.0, lat_window)\n            lat_norm = min(1.0, latency_ms / lat_window)\n            reward = max(0.0, 1.0 - w_cost * cost_norm - w_lat * lat_norm)\n            self.learning.update(task_type, chosen, reward=reward)\n            metrics.LLM_MODEL_SELECTED.labels(**_labels(), task=task_type, model=chosen, provider=provider_family).inc()\n            metrics.LLM_ESTIMATED_COST.labels(**_labels(), model=chosen, provider=provider_family).observe(\n                projected_cost\n            )\n            metrics.LLM_LATENCY.labels(**_labels()).observe(latency_ms)\n            if self.logger:\n                self.logger.log_llm_call(\n                    task_type,\n                    chosen,\n                    str(provider),\n                    tokens_in,\n                    tokens_out,\n                    float(projected_cost),\n                    latency_ms,\n                    None,\n                    True,\n                    None,\n                )\n            result = {\n                "status": "success",\n                "model": chosen,\n                "response": message,\n                "tokens": tokens_in,\n                "provider": provider,\n            }\n            trace_llm_call(\n                name=f"openrouter_{task_type}",\n                prompt=prompt,\n                response=message,\n                model=chosen,\n                metadata={"provider": provider, "task_type": task_type, "offline": False, "api_endpoint": "openrouter"},\n                latency_ms=latency_ms,\n                token_usage={\n                    "input_tokens": tokens_in,\n                    "output_tokens": tokens_out,\n                    "total_tokens": tokens_in + tokens_out,\n                },\n                cost=projected_cost,\n            )\n            # Persist semantic cache in shadow mode or production mode\n            if self.semantic_cache is not None:\n                try:\n                    import asyncio as _asyncio\n                    import threading as _threading\n\n                    sc = self.semantic_cache\n\n                    def _runner_set() -> StepResult:\n                        try:\n                            if sc is not None:\n                                _asyncio.run(sc.set(prompt, chosen, result, namespace=ns))\n                        except Exception:\n                            pass\n\n                    t_set = _threading.Thread(target=_runner_set, daemon=True)\n                    t_set.start()\n                    t_set.join()\n                    log.debug("semantic_cache_set completed for model=%s ns=%s", chosen, ns)\n                except Exception:\n                    pass\n            if self.cache and cache_key:\n                self.cache.set(cache_key, result)\n            result["cached"] = False\n            if tracker:\n                try:\n                    tracker.charge(projected_cost, task_type)\n                except Exception as charge_exc:  # pragma: no cover\n                    log.debug(\n                        "request budget charge failed (network success path) task=%s model=%s err=%s",\n                        task_type,\n                        chosen,\n                        charge_exc,\n                    )\n            return result\n        except Exception as exc:  # pragma: no cover\n            latency_ms = (time.perf_counter() - start) * 1000\n            if self.logger:\n                self.logger.log_llm_call(\n                    task_type, chosen, str(provider), tokens_in, 0, 0.0, latency_ms, None, False, None\n                )\n            log.error(\n                "OpenRouterService route failed task=%s model=%s provider=%s err=%s",\n                task_type,\n                chosen,\n                provider_family,\n                exc,\n                exc_info=True,\n            )\n            return {"status": "error", "error": str(exc), "model": chosen, "tokens": tokens_in, "provider": provider}\n'
