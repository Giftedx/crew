"""Crew comparison dashboard for visualizing performance metrics.

This module provides a dashboard for comparing crew implementations,
generating charts, and exporting results to documentation.
"""

from __future__ import annotations

import json
import logging
import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

from ultimate_discord_intelligence_bot.step_result import StepResult


if TYPE_CHECKING:
    from ultimate_discord_intelligence_bot.features.crew_analytics import CrewAnalytics


logger = logging.getLogger(__name__)


@dataclass
class DashboardMetrics:
    """Metrics for dashboard visualization."""

    crew_type: str
    success_rate: float
    avg_execution_time: float
    avg_memory_usage: float
    avg_cpu_usage: float
    total_executions: int
    successful_executions: int
    failed_executions: int
    performance_score: float
    last_execution: float | None = None


@dataclass
class ComparisonChart:
    """Chart data for crew comparison."""

    chart_type: str
    title: str
    data: dict[str, Any]
    labels: list[str]
    values: list[float]
    colors: list[str] = field(default_factory=lambda: ["#3498db", "#e74c3c", "#2ecc71", "#f39c12", "#9b59b6"])


@dataclass
class DashboardData:
    """Complete dashboard data structure."""

    timestamp: str
    metrics: list[DashboardMetrics]
    charts: list[ComparisonChart]
    summary: dict[str, Any]
    recommendations: list[str]


class CrewDashboard:
    """Dashboard for crew performance comparison and visualization."""

    def __init__(self, analytics: CrewAnalytics):
        """Initialize crew dashboard.

        Args:
            analytics: Crew analytics instance for data collection
        """
        self.analytics = analytics

    def get_dashboard_data(self) -> StepResult:
        """Get comprehensive dashboard data.

        Returns:
            StepResult: Result containing dashboard data
        """
        try:
            # Get analytics data
            analytics_result = self.analytics.get_dashboard_data()
            if not analytics_result.success:
                return StepResult.fail("Failed to get analytics data")

            analytics_data = analytics_result.data

            # Convert to dashboard metrics
            metrics = []
            for crew_type, crew_data in analytics_data["crews"].items():
                metrics.append(
                    DashboardMetrics(
                        crew_type=crew_type,
                        success_rate=crew_data["success_rate"],
                        avg_execution_time=crew_data["average_execution_time"],
                        avg_memory_usage=crew_data["average_memory_usage"],
                        avg_cpu_usage=crew_data.get("average_cpu_usage", 0.0),
                        total_executions=crew_data.get("total_executions", 0),
                        successful_executions=int(crew_data["success_rate"] * crew_data.get("total_executions", 0)),
                        failed_executions=int((1 - crew_data["success_rate"]) * crew_data.get("total_executions", 0)),
                        performance_score=crew_data["performance_score"],
                        last_execution=crew_data.get("last_execution"),
                    )
                )

            # Generate charts
            charts = self._generate_charts(metrics)

            # Generate summary
            summary = self._generate_summary(metrics)

            # Generate recommendations
            recommendations = self._generate_recommendations(metrics)

            dashboard_data = DashboardData(
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                metrics=metrics,
                charts=charts,
                summary=summary,
                recommendations=recommendations,
            )

            return StepResult.ok(
                data={
                    "dashboard_data": dashboard_data,
                    "metrics_count": len(metrics),
                    "charts_count": len(charts),
                    "recommendations_count": len(recommendations),
                }
            )

        except Exception as e:
            logger.error(f"Failed to get dashboard data: {e}")
            return StepResult.fail(f"Dashboard data retrieval failed: {e}")

    def _generate_charts(self, metrics: list[DashboardMetrics]) -> list[ComparisonChart]:
        """Generate comparison charts.

        Args:
            metrics: List of crew metrics

        Returns:
            List of comparison charts
        """
        charts: list[ComparisonChart] = []

        if not metrics:
            return charts

        # Success Rate Chart
        success_chart = ComparisonChart(
            chart_type="bar",
            title="Success Rate Comparison",
            data={
                "type": "bar",
                "data": {
                    "labels": [m.crew_type for m in metrics],
                    "datasets": [
                        {
                            "label": "Success Rate (%)",
                            "data": [m.success_rate * 100 for m in metrics],
                            "backgroundColor": ["#2ecc71" if m.success_rate > 0.8 else "#e74c3c" for m in metrics],
                            "borderColor": ["#27ae60" if m.success_rate > 0.8 else "#c0392b" for m in metrics],
                            "borderWidth": 1,
                        }
                    ],
                },
                "options": {"scales": {"y": {"beginAtZero": True, "max": 100}}},
            },
            labels=[m.crew_type for m in metrics],
            values=[m.success_rate * 100 for m in metrics],
        )
        charts.append(success_chart)

        # Execution Time Chart
        time_chart = ComparisonChart(
            chart_type="bar",
            title="Average Execution Time",
            data={
                "type": "bar",
                "data": {
                    "labels": [m.crew_type for m in metrics],
                    "datasets": [
                        {
                            "label": "Execution Time (seconds)",
                            "data": [m.avg_execution_time for m in metrics],
                            "backgroundColor": ["#3498db" for _ in metrics],
                            "borderColor": ["#2980b9" for _ in metrics],
                            "borderWidth": 1,
                        }
                    ],
                },
                "options": {"scales": {"y": {"beginAtZero": True}}},
            },
            labels=[m.crew_type for m in metrics],
            values=[m.avg_execution_time for m in metrics],
        )
        charts.append(time_chart)

        # Memory Usage Chart
        memory_chart = ComparisonChart(
            chart_type="bar",
            title="Average Memory Usage",
            data={
                "type": "bar",
                "data": {
                    "labels": [m.crew_type for m in metrics],
                    "datasets": [
                        {
                            "label": "Memory Usage (MB)",
                            "data": [m.avg_memory_usage for m in metrics],
                            "backgroundColor": ["#f39c12" for _ in metrics],
                            "borderColor": ["#e67e22" for _ in metrics],
                            "borderWidth": 1,
                        }
                    ],
                },
                "options": {"scales": {"y": {"beginAtZero": True}}},
            },
            labels=[m.crew_type for m in metrics],
            values=[m.avg_memory_usage for m in metrics],
        )
        charts.append(memory_chart)

        # Performance Score Chart
        performance_chart = ComparisonChart(
            chart_type="radar",
            title="Performance Score Comparison",
            data={
                "type": "radar",
                "data": {
                    "labels": ["Success Rate", "Speed", "Memory Efficiency", "Reliability"],
                    "datasets": [
                        {
                            "label": m.crew_type,
                            "data": [
                                m.success_rate * 100,
                                max(0, 100 - (m.avg_execution_time / 10)),  # Speed score
                                max(0, 100 - (m.avg_memory_usage / 10)),  # Memory efficiency
                                m.performance_score * 100,
                            ],
                            "backgroundColor": f"rgba({i * 50}, {100 + i * 30}, {200 - i * 40}, 0.2)",
                            "borderColor": f"rgba({i * 50}, {100 + i * 30}, {200 - i * 40}, 1)",
                            "borderWidth": 2,
                        }
                        for i, m in enumerate(metrics)
                    ],
                },
                "options": {"scales": {"r": {"beginAtZero": True, "max": 100}}},
            },
            labels=[m.crew_type for m in metrics],
            values=[m.performance_score for m in metrics],
        )
        charts.append(performance_chart)

        return charts

    def _generate_summary(self, metrics: list[DashboardMetrics]) -> dict[str, Any]:
        """Generate summary statistics.

        Args:
            metrics: List of crew metrics

        Returns:
            Summary statistics
        """
        if not metrics:
            return {}

        # Find best performers
        best_success = max(metrics, key=lambda m: m.success_rate)
        fastest = min(metrics, key=lambda m: m.avg_execution_time)
        most_efficient = min(metrics, key=lambda m: m.avg_memory_usage)
        best_performance = max(metrics, key=lambda m: m.performance_score)

        # Calculate totals
        total_executions = sum(m.total_executions for m in metrics)
        total_successful = sum(m.successful_executions for m in metrics)
        overall_success_rate = total_successful / total_executions if total_executions > 0 else 0

        return {
            "total_crews": len(metrics),
            "total_executions": total_executions,
            "total_successful": total_successful,
            "overall_success_rate": overall_success_rate,
            "best_success_rate": {"crew": best_success.crew_type, "rate": best_success.success_rate},
            "fastest_execution": {"crew": fastest.crew_type, "time": fastest.avg_execution_time},
            "most_memory_efficient": {"crew": most_efficient.crew_type, "usage": most_efficient.avg_memory_usage},
            "best_overall_performance": {
                "crew": best_performance.crew_type,
                "score": best_performance.performance_score,
            },
        }

    def _generate_recommendations(self, metrics: list[DashboardMetrics]) -> list[str]:
        """Generate recommendations based on metrics.

        Args:
            metrics: List of crew metrics

        Returns:
            List of recommendations
        """
        recommendations = []

        if not metrics:
            return ["No metrics available for recommendations"]

        # Find best overall performer
        best_performer = max(metrics, key=lambda m: m.performance_score)
        recommendations.append(
            f"🏆 Recommended for production: {best_performer.crew_type} (performance score: {best_performer.performance_score:.2f})"
        )

        # Performance analysis
        for metric in metrics:
            if metric.success_rate < 0.8:
                recommendations.append(
                    f"⚠️ {metric.crew_type}: Low success rate ({metric.success_rate:.1%}) - investigate stability issues"
                )

            if metric.avg_execution_time > 60.0:
                recommendations.append(
                    f"⚠️ {metric.crew_type}: Slow execution ({metric.avg_execution_time:.1f}s) - consider optimization"
                )

            if metric.avg_memory_usage > 500.0:
                recommendations.append(
                    f"⚠️ {metric.crew_type}: High memory usage ({metric.avg_memory_usage:.1f}MB) - consider memory optimization"
                )

        # Comparative recommendations
        if len(metrics) > 1:
            fastest = min(metrics, key=lambda m: m.avg_execution_time)
            most_efficient = min(metrics, key=lambda m: m.avg_memory_usage)

            if fastest.crew_type != best_performer.crew_type:
                recommendations.append(
                    f"⚡ For speed-critical workloads: {fastest.crew_type} (avg: {fastest.avg_execution_time:.1f}s)"
                )

            if most_efficient.crew_type != best_performer.crew_type:
                recommendations.append(
                    f"💾 For memory-constrained environments: {most_efficient.crew_type} (avg: {most_efficient.avg_memory_usage:.1f}MB)"
                )

        return recommendations

    def export_to_markdown(
        self, dashboard_data: DashboardData, filename: str = "crew_comparison_report.md"
    ) -> StepResult:
        """Export dashboard data to markdown report.

        Args:
            dashboard_data: Dashboard data to export
            filename: Output filename

        Returns:
            StepResult: Result of export operation
        """
        try:
            with open(filename, "w") as f:
                f.write("# Crew Performance Comparison Report\n\n")
                f.write(f"**Generated:** {dashboard_data.timestamp}\n\n")

                # Summary section
                f.write("## Summary\n\n")
                summary = dashboard_data.summary
                f.write(f"- **Total Crews Tested:** {summary.get('total_crews', 0)}\n")
                f.write(f"- **Total Executions:** {summary.get('total_executions', 0)}\n")
                f.write(f"- **Overall Success Rate:** {summary.get('overall_success_rate', 0):.1%}\n")
                f.write(f"- **Best Performer:** {summary.get('best_overall_performance', {}).get('crew', 'N/A')}\n\n")

                # Performance table
                f.write("## Performance Metrics\n\n")
                f.write("| Crew Type | Success Rate | Avg Time (s) | Avg Memory (MB) | Performance Score |\n")
                f.write("|-----------|--------------|--------------|-----------------|-------------------|\n")

                for metric in dashboard_data.metrics:
                    f.write(
                        f"| {metric.crew_type} | {metric.success_rate:.1%} | {metric.avg_execution_time:.2f} | {metric.avg_memory_usage:.1f} | {metric.performance_score:.2f} |\n"
                    )

                f.write("\n")

                # Recommendations
                f.write("## Recommendations\n\n")
                for i, recommendation in enumerate(dashboard_data.recommendations, 1):
                    f.write(f"{i}. {recommendation}\n")

                f.write("\n")

                # Charts section
                f.write("## Performance Charts\n\n")
                f.write("The following charts provide visual comparisons of crew performance:\n\n")

                for chart in dashboard_data.charts:
                    f.write(f"### {chart.title}\n\n")
                    f.write("```json\n")
                    f.write(json.dumps(chart.data, indent=2))
                    f.write("\n```\n\n")

                f.write("---\n")
                f.write("*Report generated by Crew Dashboard*\n")

            return StepResult.ok(data={"filename": filename, "size": len(dashboard_data.metrics)})

        except Exception as e:
            logger.error(f"Failed to export markdown report: {e}")
            return StepResult.fail(f"Markdown export failed: {e}")

    def export_to_json(self, dashboard_data: DashboardData, filename: str = "crew_comparison_data.json") -> StepResult:
        """Export dashboard data to JSON file.

        Args:
            dashboard_data: Dashboard data to export
            filename: Output filename

        Returns:
            StepResult: Result of export operation
        """
        try:
            # Convert dataclass to dict for JSON serialization
            export_data = {
                "timestamp": dashboard_data.timestamp,
                "metrics": [
                    {
                        "crew_type": m.crew_type,
                        "success_rate": m.success_rate,
                        "avg_execution_time": m.avg_execution_time,
                        "avg_memory_usage": m.avg_memory_usage,
                        "avg_cpu_usage": m.avg_cpu_usage,
                        "total_executions": m.total_executions,
                        "successful_executions": m.successful_executions,
                        "failed_executions": m.failed_executions,
                        "performance_score": m.performance_score,
                        "last_execution": m.last_execution,
                    }
                    for m in dashboard_data.metrics
                ],
                "charts": [
                    {
                        "chart_type": c.chart_type,
                        "title": c.title,
                        "data": c.data,
                        "labels": c.labels,
                        "values": c.values,
                        "colors": c.colors,
                    }
                    for c in dashboard_data.charts
                ],
                "summary": dashboard_data.summary,
                "recommendations": dashboard_data.recommendations,
            }

            with open(filename, "w") as f:
                json.dump(export_data, f, indent=2)

            return StepResult.ok(data={"filename": filename, "metrics_count": len(dashboard_data.metrics)})

        except Exception as e:
            logger.error(f"Failed to export JSON data: {e}")
            return StepResult.fail(f"JSON export failed: {e}")

    def generate_html_dashboard(
        self, dashboard_data: DashboardData, filename: str = "crew_dashboard.html"
    ) -> StepResult:
        """Generate HTML dashboard.

        Args:
            dashboard_data: Dashboard data to visualize
            filename: Output filename

        Returns:
            StepResult: Result of HTML generation
        """
        try:
            html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Crew Performance Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ text-align: center; margin-bottom: 30px; }}
        .metrics-table {{ width: 100%; border-collapse: collapse; margin-bottom: 30px; }}
        .metrics-table th, .metrics-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        .metrics-table th {{ background-color: #f2f2f2; }}
        .chart-container {{ margin: 20px 0; }}
        .recommendations {{ background-color: #f9f9f9; padding: 15px; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Crew Performance Dashboard</h1>
        <p>Generated: {dashboard_data.timestamp}</p>
    </div>

    <h2>Performance Metrics</h2>
    <table class="metrics-table">
        <tr>
            <th>Crew Type</th>
            <th>Success Rate</th>
            <th>Avg Time (s)</th>
            <th>Avg Memory (MB)</th>
            <th>Performance Score</th>
        </tr>
"""

            for metric in dashboard_data.metrics:
                html_content += f"""
        <tr>
            <td>{metric.crew_type}</td>
            <td>{metric.success_rate:.1%}</td>
            <td>{metric.avg_execution_time:.2f}</td>
            <td>{metric.avg_memory_usage:.1f}</td>
            <td>{metric.performance_score:.2f}</td>
        </tr>
"""

            html_content += """
    </table>

    <h2>Performance Charts</h2>
"""

            # Add chart containers
            for i, chart in enumerate(dashboard_data.charts):
                html_content += f"""
    <div class="chart-container">
        <h3>{chart.title}</h3>
        <canvas id="chart{i}" width="400" height="200"></canvas>
    </div>
"""

            html_content += """
    <h2>Recommendations</h2>
    <div class="recommendations">
        <ul>
"""

            for recommendation in dashboard_data.recommendations:
                html_content += f"            <li>{recommendation}</li>\n"

            html_content += """
        </ul>
    </div>

    <script>
"""

            # Add chart JavaScript
            for i, chart in enumerate(dashboard_data.charts):
                html_content += f"""
        // Chart {i}: {chart.title}
        const ctx{i} = document.getElementById('chart{i}').getContext('2d');
        new Chart(ctx{i}, {json.dumps(chart.data)});
"""

            html_content += """
    </script>
</body>
</html>
"""

            with open(filename, "w") as f:
                f.write(html_content)

            return StepResult.ok(data={"filename": filename, "charts_count": len(dashboard_data.charts)})

        except Exception as e:
            logger.error(f"Failed to generate HTML dashboard: {e}")
            return StepResult.fail(f"HTML dashboard generation failed: {e}")
