"""Hybrid retrieval combining sparse (BM25) + dense + reranking for Qdrant.

Implements state-of-the-art retrieval with:
- Sparse vectors (SPLADE++ via FastEmbed)
- Dense vectors (BGE embeddings)
- Reciprocal Rank Fusion (RRF) or Distribution-Based Score Fusion (DBSF)
- Reranking (Cohere or BGE reranker)
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any

from core.secure_config import get_config
from memory.qdrant_provider import get_qdrant_client
from obs import metrics


if TYPE_CHECKING:  # pragma: no cover
    from qdrant_client.models import ScoredPoint

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """Result from hybrid retrieval."""

    points: list[ScoredPoint]
    scores: list[float]
    reranked: bool
    fusion_method: str
    latency_ms: float
    metadata: dict[str, Any]


class HybridRetriever:
    """Hybrid retrieval with sparse + dense vectors and reranking.

    Architecture:
    1. Parallel prefetch: sparse (BM25-like SPLADE++) + dense (BGE)
    2. Fusion: RRF (Reciprocal Rank Fusion) or DBSF (Distribution-Based Score Fusion)
    3. Optional reranking: Cohere or BGE reranker for top-K

    Features:
    - Async Qdrant client
    - FastEmbed for sparse/dense embeddings
    - Per-collection configuration
    - Metrics export
    """

    def __init__(
        self,
        collection_name: str,
        fusion_method: str | None = None,
        enable_reranking: bool | None = None,
    ) -> None:
        """Initialize hybrid retriever.

        Args:
            collection_name: Qdrant collection name
            fusion_method: "rrf" or "dbsf" (default from config)
            enable_reranking: Whether to rerank results (default from config)
        """
        config = get_config()
        self.enabled = config.enable_hybrid_retrieval
        self.collection_name = collection_name
        self.fusion_method = fusion_method or config.hybrid_retrieval_fusion_method
        self.enable_reranking = enable_reranking if enable_reranking is not None else config.enable_reranker
        self.reranker_model = config.reranker_model
        self.reranker_top_k = config.reranker_top_k

        self._qdrant_client: Any = None
        self._sparse_encoder: Any = None
        self._dense_encoder: Any = None
        self._reranker: Any = None

        if self.enabled:
            self._initialize_components()
            logger.info(
                "Hybrid retriever initialized: collection=%s, fusion=%s, rerank=%s",
                collection_name,
                self.fusion_method,
                self.enable_reranking,
            )

    def _initialize_components(self) -> None:
        """Initialize Qdrant client and embedding models."""
        # Qdrant client (pooled, async-ready)
        self._qdrant_client = get_qdrant_client()

        # FastEmbed encoders (lazy-loaded)
        try:
            from fastembed import SparseTextEmbedding, TextEmbedding  # type: ignore

            # Sparse encoder (SPLADE++)
            self._sparse_encoder = SparseTextEmbedding(
                model_name="prithivida/Splade_PP_en_v1",
                cache_dir="./data/fastembed_cache",
            )

            # Dense encoder (BGE)
            self._dense_encoder = TextEmbedding(
                model_name="BAAI/bge-small-en-v1.5",
                cache_dir="./data/fastembed_cache",
            )

            logger.info("FastEmbed encoders initialized")

        except Exception as e:
            logger.warning("Failed to initialize FastEmbed encoders: %s; hybrid retrieval disabled", e)
            self.enabled = False
            return

        # Optional reranker
        if self.enable_reranking:
            self._initialize_reranker()

    def _initialize_reranker(self) -> None:
        """Initialize reranker model."""
        try:
            # Try Cohere API first (if key available)
            config = get_config()
            if config.cohere_api_key:
                import cohere  # type: ignore

                self._reranker = cohere.Client(config.cohere_api_key)
                self._reranker_type = "cohere"
                logger.info("Cohere reranker initialized")
                return

        except Exception as e:
            logger.debug("Cohere reranker unavailable: %s", e)

        try:
            # Fallback to local BGE reranker
            from fastembed.rerank.cross_encoder import TextCrossEncoder  # type: ignore

            self._reranker = TextCrossEncoder(
                model_name=self.reranker_model,
                cache_dir="./data/fastembed_cache",
            )
            self._reranker_type = "fastembed"
            logger.info("FastEmbed reranker initialized: %s", self.reranker_model)

        except Exception as e:
            logger.warning("Failed to initialize reranker: %s; reranking disabled", e)
            self.enable_reranking = False

    async def retrieve(
        self,
        query: str,
        limit: int = 10,
        filter_dict: dict[str, Any] | None = None,
    ) -> RetrievalResult:
        """Hybrid retrieval: sparse + dense + fusion + rerank.

        Args:
            query: Query text
            limit: Number of results to return
            filter_dict: Optional Qdrant filter

        Returns:
            RetrievalResult with fused and optionally reranked points
        """
        import time

        start = time.perf_counter()

        if not self.enabled:
            # Fallback to dense-only retrieval
            return await self._dense_only_retrieve(query, limit, filter_dict)

        try:
            # 1. Encode query (sparse + dense)
            sparse_embedding = await self._encode_sparse(query)
            dense_embedding = await self._encode_dense(query)

            # 2. Parallel prefetch from Qdrant
            sparse_results = await self._prefetch_sparse(sparse_embedding, limit * 2, filter_dict)
            dense_results = await self._prefetch_dense(dense_embedding, limit * 2, filter_dict)

            # 3. Fusion (RRF or DBSF)
            fused_results = self._fuse_results(sparse_results, dense_results, limit)

            # 4. Optional reranking
            if self.enable_reranking and len(fused_results) > 0:
                fused_results = await self._rerank_results(query, fused_results, limit)
                reranked = True
            else:
                reranked = False

            latency_ms = (time.perf_counter() - start) * 1000

            # Extract scores
            scores = [point.score for point in fused_results] if hasattr(fused_results[0], "score") else []

            # Metrics
            self._export_metrics(latency_ms, len(fused_results), reranked)

            return RetrievalResult(
                points=fused_results,
                scores=scores,
                reranked=reranked,
                fusion_method=self.fusion_method,
                latency_ms=latency_ms,
                metadata={
                    "sparse_count": len(sparse_results),
                    "dense_count": len(dense_results),
                    "fused_count": len(fused_results),
                },
            )

        except Exception as e:
            logger.exception("Hybrid retrieval failed: %s", e)
            # Fallback to dense-only
            return await self._dense_only_retrieve(query, limit, filter_dict)

    async def _encode_sparse(self, text: str) -> Any:
        """Encode text with sparse encoder (SPLADE++)."""
        # FastEmbed returns generator; convert to list
        embeddings = list(self._sparse_encoder.embed([text]))
        return embeddings[0] if embeddings else None

    async def _encode_dense(self, text: str) -> list[float]:
        """Encode text with dense encoder (BGE)."""
        embeddings = list(self._dense_encoder.embed([text]))
        return list(embeddings[0]) if embeddings else []

    async def _prefetch_sparse(
        self,
        sparse_embedding: Any,
        limit: int,
        filter_dict: dict[str, Any] | None,
    ) -> list[Any]:
        """Prefetch using sparse vectors."""
        try:
            # Qdrant sparse vector query
            # Note: Requires sparse vectors configured in collection
            results = self._qdrant_client.query_points(
                collection_name=self.collection_name,
                query=sparse_embedding,
                using="sparse",  # named sparse vector
                limit=limit,
                query_filter=filter_dict,
                with_payload=True,
            )
            return results.points if hasattr(results, "points") else []

        except Exception as e:
            logger.debug("Sparse prefetch failed: %s; skipping", e)
            return []

    async def _prefetch_dense(
        self,
        dense_embedding: list[float],
        limit: int,
        filter_dict: dict[str, Any] | None,
    ) -> list[Any]:
        """Prefetch using dense vectors."""
        try:
            results = self._qdrant_client.query_points(
                collection_name=self.collection_name,
                query=dense_embedding,
                using="dense",  # named dense vector (or default)
                limit=limit,
                query_filter=filter_dict,
                with_payload=True,
            )
            return results.points if hasattr(results, "points") else []

        except Exception as e:
            logger.warning("Dense prefetch failed: %s", e)
            return []

    def _fuse_results(
        self,
        sparse_results: list[Any],
        dense_results: list[Any],
        limit: int,
    ) -> list[Any]:
        """Fuse sparse and dense results using RRF or DBSF."""
        if self.fusion_method == "rrf":
            return self._rrf_fusion(sparse_results, dense_results, limit)
        elif self.fusion_method == "dbsf":
            return self._dbsf_fusion(sparse_results, dense_results, limit)
        else:
            logger.warning("Unknown fusion method %s; using RRF", self.fusion_method)
            return self._rrf_fusion(sparse_results, dense_results, limit)

    def _rrf_fusion(
        self,
        sparse_results: list[Any],
        dense_results: list[Any],
        limit: int,
        k: int = 60,
    ) -> list[Any]:
        """Reciprocal Rank Fusion (RRF).

        RRF score = sum(1 / (k + rank_i))  for each result list i
        """
        # Build rank maps: point_id -> rank (1-indexed)
        sparse_ranks = {point.id: rank + 1 for rank, point in enumerate(sparse_results)}
        dense_ranks = {point.id: rank + 1 for rank, point in enumerate(dense_results)}

        # Combine all point IDs
        all_ids = set(sparse_ranks.keys()) | set(dense_ranks.keys())

        # Compute RRF scores
        rrf_scores = {}
        for point_id in all_ids:
            score = 0.0
            if point_id in sparse_ranks:
                score += 1.0 / (k + sparse_ranks[point_id])
            if point_id in dense_ranks:
                score += 1.0 / (k + dense_ranks[point_id])
            rrf_scores[point_id] = score

        # Sort by RRF score (descending)
        sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)[:limit]

        # Build point objects with RRF scores
        # Fetch points from either list (prefer dense for payload completeness)
        id_to_point = {point.id: point for point in dense_results}
        id_to_point.update({point.id: point for point in sparse_results})

        fused = []
        for point_id in sorted_ids:
            if point_id in id_to_point:
                point = id_to_point[point_id]
                # Attach RRF score
                point.score = rrf_scores[point_id]
                fused.append(point)

        return fused

    def _dbsf_fusion(
        self,
        sparse_results: list[Any],
        dense_results: list[Any],
        limit: int,
    ) -> list[Any]:
        """Distribution-Based Score Fusion (DBSF).

        Normalizes scores using mean ± 3*std, then sums across result lists.
        """
        import numpy as np

        # Extract scores
        sparse_scores = np.array([point.score for point in sparse_results]) if sparse_results else np.array([])
        dense_scores = np.array([point.score for point in dense_results]) if dense_results else np.array([])

        # Normalize each distribution
        def normalize(scores: np.ndarray) -> np.ndarray:
            if len(scores) == 0:
                return scores
            mean = np.mean(scores)
            std = np.std(scores)
            if std == 0:
                return scores - mean  # all same score -> 0
            # Clamp to [mean - 3*std, mean + 3*std] then map to [0, 1]
            lower = mean - 3 * std
            upper = mean + 3 * std
            clamped = np.clip(scores, lower, upper)
            return (clamped - lower) / (upper - lower) if upper > lower else clamped * 0

        sparse_norm = normalize(sparse_scores)
        dense_norm = normalize(dense_scores)

        # Build score map: point_id -> sum of normalized scores
        sparse_map = {sparse_results[i].id: sparse_norm[i] for i in range(len(sparse_results))}
        dense_map = {dense_results[i].id: dense_norm[i] for i in range(len(dense_results))}

        all_ids = set(sparse_map.keys()) | set(dense_map.keys())

        dbsf_scores = {}
        for point_id in all_ids:
            dbsf_scores[point_id] = sparse_map.get(point_id, 0.0) + dense_map.get(point_id, 0.0)

        # Sort and select top-K
        sorted_ids = sorted(dbsf_scores.keys(), key=lambda x: dbsf_scores[x], reverse=True)[:limit]

        # Build points with DBSF scores
        id_to_point = {point.id: point for point in dense_results}
        id_to_point.update({point.id: point for point in sparse_results})

        fused = []
        for point_id in sorted_ids:
            if point_id in id_to_point:
                point = id_to_point[point_id]
                point.score = dbsf_scores[point_id]
                fused.append(point)

        return fused

    async def _rerank_results(
        self,
        query: str,
        results: list[Any],
        limit: int,
    ) -> list[Any]:
        """Rerank results using reranker model."""
        if not self._reranker or len(results) == 0:
            return results

        try:
            # Extract documents (payloads)
            documents = [point.payload.get("text", "") for point in results]

            if self._reranker_type == "cohere":
                # Cohere Rerank API
                response = self._reranker.rerank(
                    query=query,
                    documents=documents,
                    top_n=min(limit, self.reranker_top_k),
                    model="rerank-english-v3.0",
                )
                # Reorder results based on rerank indices
                reranked = [results[r.index] for r in response.results]
                # Update scores
                for i, r in enumerate(response.results):
                    reranked[i].score = r.relevance_score

                return reranked

            elif self._reranker_type == "fastembed":
                # FastEmbed cross-encoder reranking
                scores = list(self._reranker.rerank(query, documents))
                # Pair results with scores and sort
                scored = list(zip(results, scores, strict=False))
                scored.sort(key=lambda x: x[1], reverse=True)
                reranked = [point for point, _score in scored[:limit]]
                # Update scores
                for i, (_point, score) in enumerate(scored[:limit]):
                    reranked[i].score = float(score)

                return reranked

        except Exception as e:
            logger.warning("Reranking failed: %s; returning original results", e)
            return results

        return results

    async def _dense_only_retrieve(
        self,
        query: str,
        limit: int,
        filter_dict: dict[str, Any] | None,
    ) -> RetrievalResult:
        """Fallback: dense-only retrieval (no hybrid)."""
        import time

        start = time.perf_counter()

        try:
            embedding = await self._encode_dense(query)
            results = self._qdrant_client.query_points(
                collection_name=self.collection_name,
                query=embedding,
                limit=limit,
                query_filter=filter_dict,
                with_payload=True,
            )

            points = results.points if hasattr(results, "points") else []
            latency_ms = (time.perf_counter() - start) * 1000

            return RetrievalResult(
                points=points,
                scores=[p.score for p in points],
                reranked=False,
                fusion_method="dense_only",
                latency_ms=latency_ms,
                metadata={"fallback": True},
            )

        except Exception as e:
            logger.exception("Dense-only retrieval failed: %s", e)
            return RetrievalResult(
                points=[],
                scores=[],
                reranked=False,
                fusion_method="none",
                latency_ms=(time.perf_counter() - start) * 1000,
                metadata={"error": str(e)},
            )

    def _export_metrics(self, latency_ms: float, result_count: int, reranked: bool) -> None:
        """Export retrieval metrics."""
        try:
            labels = {
                "collection": self.collection_name,
                "fusion": self.fusion_method,
                "reranked": str(reranked),
            }

            metrics.get_metrics().histogram("hybrid_retrieval_latency_ms", latency_ms, labels=labels)
            metrics.get_metrics().histogram("hybrid_retrieval_results", result_count, labels=labels)

        except Exception as e:
            logger.debug("Failed to export retrieval metrics: %s", e)


__all__ = ["HybridRetriever", "RetrievalResult"]
