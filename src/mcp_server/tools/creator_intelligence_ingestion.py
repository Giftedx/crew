"""MCP Server Tools for Creator Intelligence Content Ingestion.

This module provides TOS-compliant tools for ingesting content from multiple platforms,
writing structured data to the Knowledge Graph and Vector DB with semantic embeddings.

Platforms:
- YouTube: Videos, live streams, channel metadata
- Twitch: VODs, clips, live streams
- Twitter/X: Posts, spaces (future)

All tools follow StepResult pattern and include provenance tracking.
"""

from __future__ import annotations

import hashlib
import logging
from dataclasses import dataclass
from datetime import datetime
from platform.core.step_result import StepResult
from typing import Any

from ingest.models import Provenance
from ingest.providers import twitch, youtube
from memory.creator_intelligence_collections import get_collection_manager
from memory.embedding_service import get_embedding_service
from memory.vector_store import VectorRecord


logger = logging.getLogger(__name__)


@dataclass
class IngestedContent:
    """Structured content after ingestion."""

    platform: str
    content_id: str
    content_type: str
    creator_id: str
    title: str
    url: str
    published_at: str | None
    duration_seconds: int | None
    transcript: str | None
    metadata: dict[str, Any]
    embedding: list[float] | None = None


class CreatorIntelligenceIngestionTools:
    """MCP tools for TOS-compliant multi-platform content ingestion.

    Features:
    - Platform-specific metadata extraction
    - Automatic transcript retrieval
    - Knowledge Graph entity creation
    - Vector DB storage with embeddings
    - Provenance tracking for compliance
    - Rate limiting and retry logic
    """

    def __init__(self, sqlite_path: str = "crew_data/ingest.db", enable_vector_storage: bool = True):
        """Initialize ingestion tools.

        Args:
            sqlite_path: Path to SQLite database for metadata storage
            enable_vector_storage: Enable automatic vector DB storage
        """
        self.sqlite_path = sqlite_path
        self.enable_vector_storage = enable_vector_storage
        if enable_vector_storage:
            self.collection_manager = get_collection_manager(enable_semantic_cache=True)
            self.embedding_service = get_embedding_service()
        else:
            self.collection_manager = None
            self.embedding_service = None

    def ingest_youtube_video(self, url: str, tenant: str, workspace: str, fetch_transcript: bool = True) -> StepResult:
        """Ingest a YouTube video with metadata and optional transcript.

        Args:
            url: YouTube video URL (youtube.com/watch?v=... or youtu.be/...)
            tenant: Tenant identifier
            workspace: Workspace identifier
            fetch_transcript: Whether to fetch available transcripts

        Returns:
            StepResult with ingested content data

        Example:
            >>> tools = CreatorIntelligenceIngestionTools()
            >>> result = tools.ingest_youtube_video(
            ...     url="https://youtube.com/watch?v=dQw4w9WgXcQ",
            ...     tenant="default",
            ...     workspace="main",
            ... )
            >>> print(result.data["content_id"])
            'dQw4w9WgXcQ'
        """
        try:
            logger.info(f"Ingesting YouTube video: {url}")
            try:
                metadata_obj = youtube.fetch_metadata(url)
            except Exception as e:
                return StepResult.fail(f"Failed to fetch YouTube metadata: {e!s}", status="retryable")
            transcript = None
            if fetch_transcript:
                try:
                    transcript = youtube.fetch_transcript(url)
                    if transcript:
                        logger.info(f"Fetched transcript ({len(transcript)} chars)")
                except Exception as e:
                    logger.warning(f"Could not fetch transcript: {e}")
            content = IngestedContent(
                platform="youtube",
                content_id=metadata_obj.id,
                content_type="episode",
                creator_id=metadata_obj.channel_id or metadata_obj.channel,
                title=metadata_obj.title,
                url=metadata_obj.url,
                published_at=metadata_obj.published_at,
                duration_seconds=int(metadata_obj.duration) if metadata_obj.duration else None,
                transcript=transcript,
                metadata={
                    "channel": metadata_obj.channel,
                    "channel_id": metadata_obj.channel_id,
                    "thumbnails": metadata_obj.thumbnails,
                },
            )
            storage_result = self._store_content(content, tenant, workspace)
            if not storage_result.success:
                logger.error(f"Storage failed: {storage_result.error}")
                return storage_result
            self._track_provenance(content, tenant, workspace)
            return StepResult.ok(
                data={
                    "platform": content.platform,
                    "content_id": content.content_id,
                    "content_type": content.content_type,
                    "creator_id": content.creator_id,
                    "title": content.title,
                    "url": content.url,
                    "published_at": content.published_at,
                    "duration_seconds": content.duration_seconds,
                    "has_transcript": bool(transcript),
                    "transcript_length": len(transcript) if transcript else 0,
                    "stored_in_vector_db": self.enable_vector_storage,
                }
            )
        except Exception as e:
            logger.error(f"YouTube ingestion failed: {e}")
            return StepResult.fail(f"Ingestion failed: {e!s}", status="retryable")

    def ingest_twitch_clip(self, url: str, tenant: str, workspace: str) -> StepResult:
        """Ingest a Twitch clip with metadata.

        Args:
            url: Twitch clip URL
            tenant: Tenant identifier
            workspace: Workspace identifier

        Returns:
            StepResult with ingested content data

        Example:
            >>> tools = CreatorIntelligenceIngestionTools()
            >>> result = tools.ingest_twitch_clip(
            ...     url="https://clips.twitch.tv/ABC123",
            ...     tenant="default",
            ...     workspace="main",
            ... )
        """
        try:
            logger.info(f"Ingesting Twitch clip: {url}")
            try:
                metadata_obj = twitch.fetch_metadata(url)
            except Exception as e:
                return StepResult.fail(f"Failed to fetch Twitch metadata: {e!s}", status="retryable")
            minimal_transcript = twitch.fetch_transcript(url)
            content = IngestedContent(
                platform="twitch",
                content_id=metadata_obj.id,
                content_type="clip",
                creator_id=metadata_obj.streamer,
                title=metadata_obj.title,
                url=metadata_obj.url,
                published_at=metadata_obj.published_at,
                duration_seconds=int(metadata_obj.duration) if metadata_obj.duration else None,
                transcript=minimal_transcript,
                metadata={"streamer": metadata_obj.streamer},
            )
            storage_result = self._store_content(content, tenant, workspace)
            if not storage_result.success:
                logger.error(f"Storage failed: {storage_result.error}")
                return storage_result
            self._track_provenance(content, tenant, workspace)
            return StepResult.ok(
                data={
                    "platform": content.platform,
                    "content_id": content.content_id,
                    "content_type": content.content_type,
                    "creator_id": content.creator_id,
                    "title": content.title,
                    "url": content.url,
                    "published_at": content.published_at,
                    "duration_seconds": content.duration_seconds,
                    "stored_in_vector_db": self.enable_vector_storage,
                }
            )
        except Exception as e:
            logger.error(f"Twitch ingestion failed: {e}")
            return StepResult.fail(f"Ingestion failed: {e!s}", status="retryable")

    def batch_ingest_youtube_channel(
        self, channel_url: str, tenant: str, workspace: str, max_videos: int = 50, fetch_transcripts: bool = True
    ) -> StepResult:
        """Ingest multiple videos from a YouTube channel.

        Args:
            channel_url: YouTube channel URL
            tenant: Tenant identifier
            workspace: Workspace identifier
            max_videos: Maximum number of videos to ingest
            fetch_transcripts: Whether to fetch transcripts for each video

        Returns:
            StepResult with batch ingestion summary

        Example:
            >>> tools = CreatorIntelligenceIngestionTools()
            >>> result = tools.batch_ingest_youtube_channel(
            ...     channel_url="https://youtube.com/@h3podcast",
            ...     tenant="default",
            ...     workspace="main",
            ...     max_videos=10,
            ... )
        """
        try:
            logger.info(f"Batch ingesting YouTube channel: {channel_url} (max {max_videos} videos)")
            try:
                from googleapiclient.discovery import build

                youtube = build("youtube", "v3", developerKey=self.youtube_api_key)
                channel_id = self._extract_channel_id(channel_url)
                if not channel_id:
                    return StepResult.fail("Invalid channel URL")
                channel_response = youtube.channels().list(part="contentDetails", id=channel_id).execute()
                if not channel_response["items"]:
                    return StepResult.fail("Channel not found")
                uploads_playlist_id = channel_response["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
                videos = []
                next_page_token = None
                while len(videos) < max_videos:
                    playlist_response = (
                        youtube.playlistItems()
                        .list(
                            part="snippet",
                            playlistId=uploads_playlist_id,
                            maxResults=min(50, max_videos - len(videos)),
                            pageToken=next_page_token,
                        )
                        .execute()
                    )
                    videos.extend(playlist_response["items"])
                    next_page_token = playlist_response.get("nextPageToken")
                    if not next_page_token:
                        break
                return StepResult.ok(
                    data={"channel_id": channel_id, "videos": videos[:max_videos], "total_found": len(videos)}
                )
            except Exception as e:
                logger.error(f"Channel listing failed: {e}")
                return StepResult.fail(f"Channel listing failed: {e!s}")
        except Exception as e:
            logger.error(f"Batch ingestion failed: {e}")
            return StepResult.fail(f"Batch ingestion failed: {e!s}", status="retryable")

    def _store_content(self, content: IngestedContent, tenant: str, workspace: str) -> StepResult:
        """Store content in Knowledge Graph and Vector DB.

        Args:
            content: Ingested content object
            tenant: Tenant identifier
            workspace: Workspace identifier

        Returns:
            StepResult with storage confirmation
        """
        try:
            if self.enable_vector_storage and content.transcript and self.embedding_service:
                embed_result = self.embedding_service.embed_text(
                    text=content.transcript, model="balanced", use_cache=True
                )
                if not embed_result.success:
                    logger.warning(f"Failed to generate embedding: {embed_result.error}")
                else:
                    embedding = embed_result.data["embedding"]
                    payload = {
                        "content_type": content.content_type,
                        "platform": content.platform,
                        "creator_id": content.creator_id,
                        "episode_id": content.content_id,
                        "title": content.title,
                        "published_at": content.published_at or "",
                        "duration_seconds": content.duration_seconds or 0,
                        "url": content.url,
                        "text": content.transcript[:1000],
                        "tenant": tenant,
                        "workspace": workspace,
                    }
                    record = VectorRecord(vector=embedding, payload=payload)
                    collection_name = "creator_episodes" if content.content_type == "episode" else "creator_segments"
                    namespace = f"{tenant}:{workspace}:{collection_name}"
                    try:
                        self.collection_manager.vector_store.upsert(namespace, [record])
                        logger.info(f"âœ… Stored in Vector DB: {namespace}")
                    except Exception as e:
                        logger.error(f"Vector storage failed: {e}")
            return StepResult.ok(
                data={
                    "stored_metadata": True,
                    "stored_in_vector_db": self.enable_vector_storage and bool(content.transcript),
                }
            )
        except Exception as e:
            logger.error(f"Content storage failed: {e}")
            return StepResult.fail(f"Storage failed: {e!s}", status="retryable")

    def _track_provenance(self, content: IngestedContent, tenant: str, workspace: str) -> None:
        """Track provenance information for compliance.

        Args:
            content: Ingested content object
            tenant: Tenant identifier
            workspace: Workspace identifier
        """
        try:
            content_hash = self._compute_content_hash(content)
            Provenance(
                id=None,
                content_id=content.content_id,
                source_url=content.url,
                source_type=content.platform,
                retrieved_at=datetime.utcnow().isoformat(),
                license="platform_tos",
                terms_url=self._get_platform_tos_url(content.platform),
                consent_flags="public_content",
                checksum_sha256=content_hash,
            )
            logger.info(f"Provenance tracked: {content.platform}/{content.content_id}")
        except Exception as e:
            logger.warning(f"Provenance tracking failed: {e}")

    @staticmethod
    def _compute_content_hash(content: IngestedContent) -> str:
        """Compute SHA256 hash of content for provenance.

        Args:
            content: Ingested content object

        Returns:
            SHA256 hash string
        """
        hash_input = f"{content.platform}:{content.content_id}:{content.title}:{content.url}"
        return hashlib.sha256(hash_input.encode()).hexdigest()

    @staticmethod
    def _get_platform_tos_url(platform: str) -> str:
        """Get platform Terms of Service URL.

        Args:
            platform: Platform name

        Returns:
            TOS URL string
        """
        tos_urls = {
            "youtube": "https://www.youtube.com/t/terms",
            "twitch": "https://www.twitch.tv/p/legal/terms-of-service/",
            "twitter": "https://twitter.com/tos",
        }
        return tos_urls.get(platform, "")


def get_ingestion_tools(
    sqlite_path: str = "crew_data/ingest.db", enable_vector_storage: bool = True
) -> CreatorIntelligenceIngestionTools:
    """Factory function to create ingestion tools instance.

    Args:
        sqlite_path: Path to SQLite database
        enable_vector_storage: Enable vector DB integration

    Returns:
        Initialized ingestion tools instance
    """
    return CreatorIntelligenceIngestionTools(sqlite_path=sqlite_path, enable_vector_storage=enable_vector_storage)
