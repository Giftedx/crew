# Ultimate Discord Intelligence Bot - Environment Configuration Template
# Copy this file to .env and customize for your deployment

# ====== REQUIRED API KEYS ======
# At least one of these is required for LLM functionality
# Prefer creating org / project scoped keys with least privileges.
OPENAI_API_KEY=sk-your-openai-key-here
OPENROUTER_API_KEY=sk-your-openrouter-key-here
# Attribution (recommended by OpenRouter)
OPENROUTER_REFERER=https://yourapp.example.com
OPENROUTER_TITLE=Ultimate Discord Intelligence Bot

# ====== DISCORD INTEGRATION ======
# Required for Discord bot functionality and notifications
DISCORD_BOT_TOKEN=your-discord-bot-token-here
# Note: variable names must match code expectations below (no _URL suffix)
# Public/notifications webhook (used by tools and pipeline)
DISCORD_WEBHOOK=https://discord.com/api/webhooks/YOUR_WEBHOOK_HERE
# Private/restricted webhook (e.g., ops/alerts, lower traffic)
DISCORD_PRIVATE_WEBHOOK=https://discord.com/api/webhooks/YOUR_PRIVATE_WEBHOOK_HERE
# Optional: Alertmanager → Discord adapter webhook
DISCORD_ALERT_WEBHOOK=https://discord.com/api/webhooks/YOUR_ALERTS_WEBHOOK

# ====== WEBHOOK SECURITY ======
# CRITICAL: Set these to secure random values (never use "CHANGE_ME")
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
WEBHOOK_SECRET_DEFAULT=your-secure-webhook-secret-here
# Optional additional webhook secrets for rotation (support multi-secret validation)
WEBHOOK_SECRET_BACKUP=your-backup-webhook-secret-here

# ====== VECTOR DATABASE ======
# Qdrant for semantic search and memory storage
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your-qdrant-api-key-here
QDRANT_PREFER_GRPC=false
QDRANT_GRPC_PORT=6334

# ====== OPTIONAL EXTERNAL SERVICES ======
# Fact-checking and content analysis
GOOGLE_API_KEY=your-google-api-key-here
PERSPECTIVE_API_KEY=your-perspective-api-key-here
SERPLY_API_KEY=your-serply-api-key-here
EXA_API_KEY=your-exa-api-key-here
PERPLEXITY_API_KEY=your-perplexity-api-key-here
WOLFRAM_ALPHA_APP_ID=your-wolfram-alpha-app-id-here

# Social Media APIs
# Reddit API credentials (for RedditAPITool)
REDDIT_CLIENT_ID=your-reddit-client-id-here
REDDIT_CLIENT_SECRET=your-reddit-client-secret-here
REDDIT_USER_AGENT=UltimateDiscordIntelligenceBot/1.0

# Twitter/X API credentials (for TwitterAPITool)
TWITTER_BEARER_TOKEN=your-twitter-bearer-token-here
TWITTER_CONSUMER_KEY=your-twitter-consumer-key-here
TWITTER_CONSUMER_SECRET=your-twitter-consumer-secret-here
TWITTER_ACCESS_TOKEN=your-twitter-access-token-here
TWITTER_ACCESS_TOKEN_SECRET=your-twitter-access-token-secret-here

# ====== CORE SYSTEM FEATURES ======
SERVICE_NAME=ultimate-discord-intel
ENVIRONMENT=development
ENABLE_API=true
ENABLE_TRACING=true
ENABLE_PROMETHEUS_ENDPOINT=true
PROMETHEUS_ENDPOINT_PATH=/metrics
ENABLE_HTTP_METRICS=true
ENABLE_PROFILING=true
ENABLE_ADVANCED_CACHE=true

# ====== A2A JSON-RPC ADAPTER ======
# Expose the A2A adapter routes under /a2a
ENABLE_A2A_API=true
# Optional: require API key and set one or more comma-separated keys
ENABLE_A2A_API_KEY=true
A2A_API_KEY=your-a2a-api-key,another-key
# Optional streaming demo route under /a2a/stream-demo
ENABLE_A2A_STREAMING_DEMO=true

# Optional A2A tenancy headers for scoping calls
A2A_TENANT_ID=
A2A_WORKSPACE_ID=

# A2A skills: enable all by default for full stack
ENABLE_A2A_SKILL_SUMMARIZE=true
ENABLE_A2A_SKILL_RAG_OFFLINE=true
ENABLE_A2A_SKILL_RAG_VECTOR=true
ENABLE_A2A_SKILL_RAG_INGEST=true
ENABLE_A2A_SKILL_RAG_INGEST_URL=true
ENABLE_A2A_SKILL_RAG_HYBRID=true
ENABLE_A2A_SKILL_RESEARCH_BRIEF=true
ENABLE_A2A_SKILL_RESEARCH_BRIEF_MULTI=true
ENABLE_A2A_SKILL_RESEARCH_AND_BRIEF_MULTI=true

# ====== CONTENT PROCESSING ======
ENABLE_CACHE_GLOBAL=true
ENABLE_CACHE_TRANSCRIPT=true
ENABLE_CACHE_VECTOR=true

# ====== REINFORCEMENT LEARNING ======
ENABLE_RL_GLOBAL=true
ENABLE_RL_ROUTING=true
ENABLE_RL_PROMPT=true
ENABLE_RL_RETRIEVAL=true
ENABLE_RL_PLUGIN=true
ENABLE_RL_LINTS=true
ENABLE_RL_SHADOW=true
ENABLE_RL_SHADOW_EVAL=true
ENABLE_EXPERIMENT_HARNESS=true

# ====== DISCORD INTEGRATIONS ======
ENABLE_DISCORD_ARCHIVER=true
ENABLE_DISCORD_COMMANDS=true
# Also support code paths that check this specific flag name
ENABLE_DISCORD_USER_COMMANDS=true
ENABLE_DISCORD_MONITOR=true

# ====== SECURITY AND PRIVACY ======
ENABLE_PII_DETECTION=true
ENABLE_CONTENT_MODERATION=true
ENABLE_RATE_LIMITING=true
ENABLE_AUDIT_LOGGING=true
ENABLE_TENANCY_STRICT=false
ENABLE_GROUNDING=true
ENABLE_RAG_CONTEXT=true
ENABLE_VECTOR_SEARCH=true
ENABLE_INGEST_YOUTUBE=true
ENABLE_INGEST_TWITCH=true
ENABLE_INGEST_TIKTOK=true
ENABLE_INGEST_CONCURRENT=true
ENABLE_INGEST_STRICT=false
ENABLE_YOUTUBE_CHANNEL_BACKFILL_AFTER_INGEST=false
ENABLE_SOCIAL_INTEL=true
ENABLE_EXPERIMENTAL_DEPTH=true

# Web Automation
ENABLE_PLAYWRIGHT=false
PLAYWRIGHT_HEADLESS=true
PLAYWRIGHT_BROWSER=chromium

# LlamaIndex Integration
LLAMAINDEX_PERSIST_DIR=./data/llamaindex
LLAMAINDEX_CHUNK_SIZE=1024
LLAMAINDEX_CHUNK_OVERLAP=20

# ====== PERFORMANCE TUNING ======
MAX_WORKERS=4
CACHE_TTL_SECONDS=3600
VECTOR_BATCH_SIZE=100
RATE_LIMIT_RPS=10
RATE_LIMIT_BURST=20

# ====== HTTP AND NETWORKING ======
ENABLE_HTTP_RETRY=true
HTTP_TIMEOUT=30
HTTP_MAX_RETRIES=3
# Preferred granular: RETRY_MAX_ATTEMPTS used by core.http_utils (overrides config/retry.yaml)
RETRY_MAX_ATTEMPTS=3
HTTP_BACKOFF_BASE=0.5
HTTP_BACKOFF_MAX=8
RATE_LIMIT_REDIS_URL=

# ====== DEVELOPMENT AND DEBUG ======
DEBUG=false
LOG_LEVEL=INFO
DEBUG_TOOLS=true
ENABLE_CREW_CONFIG_VALIDATION=false
ENABLE_CREW_STEP_VERBOSE=false

# ====== FEATURE FLAGS (COMPLETE LIST) ======
# Caching / Semantic Cache
# Enable tenant-scoped semantic cache wrappers and shadow modes
ENABLE_GPTCACHE=true
ENABLE_SEMANTIC_CACHE=true
ENABLE_SEMANTIC_CACHE_SHADOW=true
ENABLE_GPTCACHE_ANALYSIS_SHADOW=true
SEMANTIC_CACHE_PROMOTION_THRESHOLD=0.9
ENABLE_SEMANTIC_CACHE_PROMOTION=true

# Prompt Compression
ENABLE_PROMPT_COMPRESSION=true
ENABLE_LLMLINGUA=true
ENABLE_LLMLINGUA_SHADOW=true
PROMPT_COMPRESSION_MAX_TOKENS=1500
LLMLINGUA_TARGET_RATIO=0.35
LLMLINGUA_MIN_TOKENS=600

# Transcript Compression (pipeline-level)
ENABLE_TRANSCRIPT_COMPRESSION=true
TRANSCRIPT_COMPRESSION_MIN_TOKENS=1200
TRANSCRIPT_COMPRESSION_TARGET_RATIO=0.35
TRANSCRIPT_COMPRESSION_MAX_TOKENS=

# Graph Memory (GraphRAG-style)
ENABLE_GRAPH_MEMORY=true

# LangGraph Pilot (experimental)
ENABLE_LANGGRAPH_PILOT=true
ENABLE_LANGGRAPH_PILOT_API=true

# HippoRAG 2 Continual Memory (experimental)
ENABLE_HIPPORAG_MEMORY=true
# Legacy flag (still supported):
ENABLE_HIPPORAG_CONTINUAL_MEMORY=true
# Optional HippoRAG configuration
HIPPORAG_LLM_MODEL=gpt-4o-mini
HIPPORAG_LLM_BASE_URL=
HIPPORAG_EMBEDDING_MODEL=nvidia/NV-Embed-v2
HIPPORAG_EMBEDDING_BASE_URL=
HIPPORAG_STORAGE=crew_data/Processing/hipporag_memory

# Adaptive Routing (Ax)
ENABLE_AX_ROUTING=true

# Semantic Router (Optional - requires: pip install -e '.[semantic_router]')
# Intelligent query routing based on semantic similarity
ENABLE_SEMANTIC_ROUTER=false

# Vowpal Wabbit Online Bandit (experimental)
ENABLE_VW_BANDIT=true
ENABLE_VOWPAL_WABBIT_BANDIT=true

# Bandit Routing (classical + contextual)
ENABLE_BANDIT_ROUTING=true
ENABLE_BANDIT_TENANT=true
BANDIT_PRIOR_ALPHA=1.0
BANDIT_PRIOR_BETA=1.0
BANDIT_MIN_EPSILON=0.0
BANDIT_RESET_ENTROPY_THRESHOLD=0.05
BANDIT_RESET_ENTROPY_WINDOW=50
ENABLE_BANDIT_PERSIST=false
BANDIT_STATE_DIR=./bandit_state

# Contextual Bandit (LinUCB) hybrid routing
ENABLE_CONTEXTUAL_BANDIT=true
LINUCB_DIMENSION=16
ENABLE_CONTEXTUAL_HYBRID=true
LINUCB_RECOMPUTE_INTERVAL=0
FEATURE_QUALITY_MIN=0.5
FEATURE_MIN_NORM=0.0
FEATURE_MAX_NORM=10.0

# Trajectory Evaluation & AgentEvals
ENABLE_TRAJECTORY_EVALUATION=1
ENABLE_ENHANCED_CREW_EVALUATION=1
ENABLE_TRAJECTORY_MATCHING=1
ENABLE_TRAJECTORY_EVALUATION_CACHE=1
ENABLE_TRAJECTORY_EVALUATION_METRICS=1
ENABLE_TRAJECTORY_SHADOW_EVALUATION=1
ENABLE_AGENT_EVALS=true

# MCP Servers & Tools
ENABLE_MCP_MEMORY=true
ENABLE_MCP_ROUTER=true
ENABLE_MCP_OBS=true
ENABLE_MCP_KG=true
ENABLE_MCP_INGEST=true
ENABLE_MCP_HTTP=true
ENABLE_MCP_A2A=true
ENABLE_MCP_CALL_TOOL=true
# CrewAI MCP integration flags
ENABLE_MCP_CREWAI=true
ENABLE_MCP_CREWAI_EXECUTION=true
MCP_HTTP_ALLOWLIST=api.github.com,raw.githubusercontent.com

# API / Server feature flags
ENABLE_PIPELINE_RUN_API=true
ENABLE_CORS=true
CORS_ALLOW_ORIGINS=http://localhost:5173,http://127.0.0.1:5173
ENABLE_ACTIVITIES_ECHO=true

# Discord runtime toggles
ENABLE_DISCORD_GATEWAY=true
AUTO_FALLBACK_HEADLESS=true
ENABLE_AUTO_FOLLOW_UPLOADER=true
ENABLE_UPLOADER_BACKFILL=true

# ====== ROUTING & REWARD WEIGHTS ======
# Default download quality if not provided by command/context
DEFAULT_DOWNLOAD_QUALITY=1080p
# RL reward shaping weights and latency window (ms)
REWARD_COST_WEIGHT=0.5
REWARD_LATENCY_WEIGHT=0.5
REWARD_LATENCY_MS_WINDOW=2000

# ====== DATABASE CONFIGURATION ======
DATABASE_URL=sqlite:///crew.db
MEMORY_DB_PATH=./memory.db
ARCHIVE_DB_PATH=./data/archive_manifest.db
TRUST_TRACKER_PATH=./data/trustworthiness.json

# ====== MEMORY MANAGEMENT ======
ENABLE_MEMORY_COMPACTION=false
ENABLE_MEMORY_TTL=false
MEMORY_TTL_SECONDS=0

# ====== CREWAI PATHS ======
# Customize CrewAI working directories (optional)
# CREWAI_BASE_DIR=~/crew_data
# CREWAI_DOWNLOADS_DIR=~/crew_data/Downloads
# CREWAI_CONFIG_DIR=~/crew_data/Config
# CREWAI_LOGS_DIR=~/crew_data/Logs
# CREWAI_PROCESSING_DIR=~/crew_data/Processing
# CREWAI_TEMP_DIR=~/crew_data/temp
# CREWAI_YTDLP_DIR=./yt-dlp
# CREWAI_YTDLP_CONFIG=./yt-dlp/config/crewai-system.conf
# CREWAI_YTDLP_ARCHIVE=./yt-dlp/archives/crewai_downloads.txt

# ====== CREWAI ENTERPRISE & TRACING ======
# CrewAI Plus/Enterprise Integration
# CREWAI_API_KEY=your-crewai-enterprise-api-key-here
# CREWAI_PROJECT_ID=your-project-id-here
CREWAI_ENABLE_TRACING=true
CREWAI_SAVE_TRACES=true
CREWAI_TRACES_DIR=crew_data/Logs/traces

# Disable CrewAI telemetry (PostHog analytics) to reduce log noise
CREWAI_DISABLE_TELEMETRY=1
TELEMETRY_OPT_OUT=1

# CrewAI Embedder Configuration
CREW_EMBEDDER_PROVIDER=openai
# CREW_EMBEDDER_CONFIG_JSON={"config": {"dimension": 1536, "api_key": "your-key"}}

# CrewAI Performance Settings
CREW_MAX_RPM=10

# ====== OPENTELEMETRY TRACING ======
# OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268/api/traces
# OTEL_EXPORTER_OTLP_HEADERS=authorization=Bearer your-token-here
# OTEL_TRACES_SAMPLER=parentbased_traceidratio
# OTEL_TRACES_SAMPLER_ARG=0.1

# ====== ARCHIVE API ======
ARCHIVE_API_TOKEN=your-archive-api-token-here

# ====== PLUGIN SYSTEM ======
ENABLE_PLUGINS=true
PLUGIN_TRUST_LEVEL=verified
PLUGIN_DIR=./plugins
PLUGIN_MAX_EXECUTION_SECONDS=30

# ====== ADDITIONAL INTEGRATIONS ======
# Redis for caching (optional)
REDIS_URL=redis://localhost:6379
REDIS_TLS=false
REDIS_USERNAME=
REDIS_PASSWORD=

# ARQ Queue (Redis-based async task queue)
ARQ_REDIS_URL=redis://localhost:6379/1

# Neo4j Knowledge Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-neo4j-password-here

# PostgreSQL for metadata (optional)
POSTGRES_URL=postgresql://user:password@localhost:5432/ultimate_discord_bot
POSTGRES_POOL_SIZE=10
POSTGRES_POOL_TIMEOUT=30

###############################################
#  LOCAL MODEL STORAGE & INFERENCE (ADDED)
###############################################
# Redirect Hugging Face cache (models, datasets, hub refs) to large mounted volume
HF_HOME=/mnt/f/hf_cache
TRANSFORMERS_CACHE=/mnt/f/hf_cache/transformers
HF_DATASETS_CACHE=/mnt/f/hf_cache/datasets

# Preferred local models (ordered by general reasoning priority)
# Adjust or prune to save disk space.
PRIMARY_MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct
SECONDARY_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
FALLBACK_SMALL_MODEL_ID=microsoft/Phi-3.5-mini-instruct
CODE_MODEL_ID=bigcode/starcoder2-7b
MATH_MODEL_ID=mistralai/Mathstral-7B-v0.1

# Quantized variants (download scripts will map these to /mnt/f/models/<dir>)
# Use these paths when you want to force local quantized weights instead of HF repo names.
PRIMARY_MODEL_LOCAL=/mnt/f/models/llama3.1-8b-instruct-awq
SECONDARY_MODEL_LOCAL=/mnt/f/models/qwen2.5-7b-instruct-awq
FALLBACK_SMALL_MODEL_LOCAL=/mnt/f/models/phi-3.5-mini

# vLLM runtime tuning (single 8GB GPU)
VLLM_MODEL_ID=${PRIMARY_MODEL_ID}
VLLM_MAX_MODEL_LEN=8192
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_WORKER_USE_RAY=0
# Choose among: awq, gptq, bitsandbytes, none (auto-detect quantized formats)
VLLM_QUANTIZATION=awq
# Reduce KV cache scale for long-context if VRAM pressure occurs (0.95..0.60 typical)
VLLM_KV_CACHE_CPU_OFFLOAD=false

# Optional memory optimizations
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
TORCH_CUDA_ARCH_LIST=8.6

# Download concurrency (used by helper scripts)
MODEL_DOWNLOAD_WORKERS=4

ENABLE_EXPERIMENTAL_DEPTH=1
ENABLE_SOCIAL_INTEL=1
ENABLE_ADVANCED_PERF=1

###############################################
#  END LOCAL MODEL STORAGE & INFERENCE SECTION
###############################################

# ====== SETUP INSTRUCTIONS ======
#
# 1. Discord Bot Token:
#    - Go to https://discord.com/developers/applications
#    - Create New Application → Bot → Copy Token
#    - Enable "Message Content Intent" and "Server Members Intent"
#
# 2. OpenAI API Key:
#    - Visit https://platform.openai.com/api-keys
#    - Create new API key
#
# 3. Qdrant Setup:
#    - Local: docker run -p 6333:6333 qdrant/qdrant
#    - Cloud: Visit https://cloud.qdrant.io
#
# 4. Discord Webhooks (optional):
#    - Server Settings → Integrations → Webhooks
#    - Create webhook for notifications
#    - Set DISCORD_WEBHOOK (and optionally DISCORD_PRIVATE_WEBHOOK) in your .env
#
###############################################
#  SECURITY & OPERATIONAL GUIDANCE
#  (Do NOT commit a real .env to source control)
###############################################
# Rotate secrets at least every 90 days (or on suspicion of compromise).
# Use separate keys per environment (dev/staging/prod).
# Never reuse Discord bot tokens across servers.
# Prefer ephemeral / short-lived API keys where provider supports them.
# For rotation: set WEBHOOK_SECRET_BACKUP in advance, deploy, then promote it
# to WEBHOOK_SECRET_DEFAULT and generate a new backup.
# Restrict outbound network for the process to required domains only.
# Consider using a secrets manager (Vault, AWS Secrets Manager, GCP Secret Manager)
# and injecting via environment at runtime instead of storing in .env files.

# 5. Start the bot:
#    make dev
#    python -m ultimate_discord_intelligence_bot.setup_cli run discord
#    ./ops/deployment/scripts/deploy.sh (container / orchestrated deploy)
