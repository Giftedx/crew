name: Golden Regression Gate

on:
  pull_request:

jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install
        run: pip install -e .[dev]
      - name: Run golden suite
        run: |
          python -m ultimate_discord_intelligence_bot.services.eval_harness run \
            --dataset datasets/golden/v1/analyze_claim.jsonl \
            --task analyze_claim --out out/eval/analyze_claim_v1.json
      - name: Compare baseline
        shell: bash
        run: |
          python - <<'PY'
          import json, yaml, sys
          from pathlib import Path

          result_path = Path('out/eval/analyze_claim_v1.json')
          baseline_path = Path('benchmarks/baselines.yaml')
          if not result_path.exists():
              print(f"Missing results file: {result_path}")
              sys.exit(1)
          res = json.load(result_path.open())
          base = yaml.safe_load(baseline_path.open())

          quality = res['aggregates']['quality']
          cost = res['aggregates']['cost_usd']
          lat = res['aggregates']['latency_ms']
          bench = base['analyze_claim_v1']

          print(f"delta quality {quality - bench['quality']:.3f}")
          if not (quality >= bench['quality'] - 0.5 and cost <= bench['cost_usd'] + 0.0 and lat <= bench['latency_ms'] + 50):
              print("Baseline comparison failed: thresholds exceeded")
              print(f"quality: {quality} (baseline {bench['quality']}) cost: {cost} (baseline {bench['cost_usd']}) latency: {lat} (baseline {bench['latency_ms']})")
              sys.exit(1)
          PY
      - uses: actions/upload-artifact@v4
        with:
          name: eval-report
          path: out/eval
